{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVxxvpq2pGF"
      },
      "source": [
        "# Previsione della Qualità dell'Aria - **Preparazione dei Dati**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm3Q39LW2Z4t"
      },
      "source": [
        "**Progetto di Data Intensive**  \n",
        "**Autore:** Martin Tomassi, Jacopo Vasi  \n",
        "**Email:** martin.tomassi@studio.unibo.it , jacopo.vasi@studio.unibo.it  \n",
        "**Corso:** Data Intensive, Università di Bologna  \n",
        "**Data:** Aprile 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnXzcSTc2Z4u"
      },
      "source": [
        "## Caricamento dei Datasets ed Import Librerie\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts2wiQjcg4BV",
        "outputId": "0c16ba0d-3710-43a3-d5f8-cb6f80a2f59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Google Colab\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Git LFS initialized.\n",
            "fatal: destination path 'Data_Project' already exists and is not an empty directory.\n",
            "\n",
            "Estratti:\n",
            "\n",
            "datasets/india/CG004.csv\n",
            "datasets/india/GJ015.csv\n",
            "datasets/india/RJ012.csv\n",
            "datasets/india/WB014.csv\n",
            "datasets/india/CG008.csv\n",
            "datasets/india/UK001.csv\n",
            "datasets/india/TG008.csv\n",
            "datasets/india/DL018.csv\n",
            "datasets/india/OR002.csv\n",
            "datasets/india/AP006.csv\n",
            "datasets/india/MH003.csv\n",
            "datasets/india/WB008.csv\n",
            "datasets/india/MH005.csv\n",
            "datasets/india/DL001.csv\n",
            "datasets/india/RJ011.csv\n",
            "datasets/india/MH010.csv\n",
            "datasets/india/MH032.csv\n",
            "datasets/india/HR021.csv\n",
            "datasets/india/RJ004.csv\n",
            "datasets/india/WB003.csv\n",
            "datasets/india/AS009.csv\n",
            "datasets/india/HR001.csv\n",
            "datasets/india/CG002.csv\n",
            "datasets/india/DL032.csv\n",
            "datasets/india/KA029.csv\n",
            "datasets/india/RJ008.csv\n",
            "datasets/india/MN001.csv\n",
            "datasets/india/DL005.csv\n",
            "datasets/india/UP012.csv\n",
            "datasets/india/GJ003.csv\n",
            "datasets/india/RJ021.csv\n",
            "datasets/india/HR027.csv\n",
            "datasets/india/KA026.csv\n",
            "datasets/india/PB003.csv\n",
            "datasets/india/BR022.csv\n",
            "datasets/india/RJ030.csv\n",
            "datasets/india/UP013.csv\n",
            "datasets/india/KA003.csv\n",
            "datasets/india/CG009.csv\n",
            "datasets/india/TG001.csv\n",
            "datasets/india/RJ017.csv\n",
            "datasets/india/MP007.csv\n",
            "datasets/india/RJ023.csv\n",
            "datasets/india/DL037.csv\n",
            "datasets/india/MH025.csv\n",
            "datasets/india/OR010.csv\n",
            "datasets/india/RJ031.csv\n",
            "datasets/india/RJ033.csv\n",
            "datasets/india/UP044.csv\n",
            "datasets/india/BR033.csv\n",
            "datasets/india/GJ014.csv\n",
            "datasets/india/WB002.csv\n",
            "datasets/india/WB011.csv\n",
            "datasets/india/UP051.csv\n",
            "datasets/india/PB005.csv\n",
            "datasets/india/TN016.csv\n",
            "datasets/india/GJ001.csv\n",
            "datasets/india/MH039.csv\n",
            "datasets/india/OR004.csv\n",
            "datasets/india/KA036.csv\n",
            "datasets/india/KL007.csv\n",
            "datasets/india/TR002.csv\n",
            "datasets/india/AS002.csv\n",
            "datasets/india/HR007.csv\n",
            "datasets/india/UP024.csv\n",
            "datasets/india/MH038.csv\n",
            "datasets/india/MP019.csv\n",
            "datasets/india/UP032.csv\n",
            "datasets/india/HR008.csv\n",
            "datasets/india/HR017.csv\n",
            "datasets/india/BR012.csv\n",
            "datasets/india/HR025.csv\n",
            "datasets/india/MH030.csv\n",
            "datasets/india/DL024.csv\n",
            "datasets/india/MH016.csv\n",
            "datasets/india/UP022.csv\n",
            "datasets/india/UP017.csv\n",
            "datasets/india/BR020.csv\n",
            "datasets/india/MP018.csv\n",
            "datasets/india/MH034.csv\n",
            "datasets/india/UP010.csv\n",
            "datasets/india/KA020.csv\n",
            "datasets/india/BR007.csv\n",
            "datasets/india/CG006.csv\n",
            "datasets/india/UP026.csv\n",
            "datasets/india/OR003.csv\n",
            "datasets/india/MH041.csv\n",
            "datasets/india/MH008.csv\n",
            "datasets/india/CG001.csv\n",
            "datasets/india/KA016.csv\n",
            "datasets/india/MP006.csv\n",
            "datasets/india/MH029.csv\n",
            "datasets/india/MP021.csv\n",
            "datasets/india/RJ002.csv\n",
            "datasets/india/UP029.csv\n",
            "datasets/india/AP008.csv\n",
            "datasets/india/RJ010.csv\n",
            "datasets/india/JK001.csv\n",
            "datasets/india/MH027.csv\n",
            "datasets/india/BR030.csv\n",
            "datasets/india/DL034.csv\n",
            "datasets/india/KA039.csv\n",
            "datasets/india/MH040.csv\n",
            "datasets/india/PB008.csv\n",
            "datasets/india/AP004.csv\n",
            "datasets/india/DL003.csv\n",
            "datasets/india/RJ013.csv\n",
            "datasets/india/UP052.csv\n",
            "datasets/india/DL006.csv\n",
            "datasets/india/GJ016.csv\n",
            "datasets/india/TN004.csv\n",
            "datasets/india/BR004.csv\n",
            "datasets/india/OR012.csv\n",
            "datasets/india/MH024.csv\n",
            "datasets/india/KA001.csv\n",
            "datasets/india/HR030.csv\n",
            "datasets/india/TG009.csv\n",
            "datasets/india/MH023.csv\n",
            "datasets/india/UP027.csv\n",
            "datasets/india/BR031.csv\n",
            "datasets/india/GJ002.csv\n",
            "datasets/india/AS004.csv\n",
            "datasets/india/UP019.csv\n",
            "datasets/india/TN011.csv\n",
            "datasets/india/KL004.csv\n",
            "datasets/india/OR005.csv\n",
            "datasets/india/DL021.csv\n",
            "datasets/india/CH003.csv\n",
            "datasets/india/AP010.csv\n",
            "datasets/india/HR023.csv\n",
            "datasets/india/TN002.csv\n",
            "datasets/india/RJ014.csv\n",
            "datasets/india/RJ006.csv\n",
            "datasets/india/MP022.csv\n",
            "datasets/india/PB004.csv\n",
            "datasets/india/AP003.csv\n",
            "datasets/india/DL030.csv\n",
            "datasets/india/RJ029.csv\n",
            "datasets/india/WB005.csv\n",
            "datasets/india/KA022.csv\n",
            "datasets/india/KL002.csv\n",
            "datasets/india/DL017.csv\n",
            "datasets/india/UP049.csv\n",
            "datasets/india/UP055.csv\n",
            "datasets/india/UP028.csv\n",
            "datasets/india/GJ013.csv\n",
            "datasets/india/UP054.csv\n",
            "datasets/india/UP050.csv\n",
            "datasets/india/ML002.csv\n",
            "datasets/india/RJ024.csv\n",
            "datasets/india/AP009.csv\n",
            "datasets/india/DL016.csv\n",
            "datasets/india/UP021.csv\n",
            "datasets/india/UP020.csv\n",
            "datasets/india/KA037.csv\n",
            "datasets/india/KA038.csv\n",
            "datasets/india/CG012.csv\n",
            "datasets/india/GJ011.csv\n",
            "datasets/india/RJ007.csv\n",
            "datasets/india/TG004.csv\n",
            "datasets/india/DL004.csv\n",
            "datasets/india/RJ022.csv\n",
            "datasets/india/CG007.csv\n",
            "datasets/india/GJ017.csv\n",
            "datasets/india/DL036.csv\n",
            "datasets/india/BR006.csv\n",
            "datasets/india/OR007.csv\n",
            "datasets/india/BR021.csv\n",
            "datasets/india/OR006.csv\n",
            "datasets/india/TN021.csv\n",
            "datasets/india/UP053.csv\n",
            "datasets/india/BR027.csv\n",
            "datasets/india/PB007.csv\n",
            "datasets/india/MP003.csv\n",
            "datasets/india/TG002.csv\n",
            "datasets/india/UP002.csv\n",
            "datasets/india/TN022.csv\n",
            "datasets/india/RJ027.csv\n",
            "datasets/india/HR005.csv\n",
            "datasets/india/KA004.csv\n",
            "datasets/india/HR013.csv\n",
            "datasets/india/UP001.csv\n",
            "datasets/india/TN006.csv\n",
            "datasets/india/GJ008.csv\n",
            "datasets/india/UP035.csv\n",
            "datasets/india/MH001.csv\n",
            "datasets/india/AP001.csv\n",
            "datasets/india/HR028.csv\n",
            "datasets/india/KA017.csv\n",
            "datasets/india/TN014.csv\n",
            "datasets/india/MH017.csv\n",
            "datasets/india/MH028.csv\n",
            "datasets/india/HR024.csv\n",
            "datasets/india/MH020.csv\n",
            "datasets/india/MP015.csv\n",
            "datasets/india/TG007.csv\n",
            "datasets/india/KA015.csv\n",
            "datasets/india/GJ012.csv\n",
            "datasets/india/WB010.csv\n",
            "datasets/india/HR026.csv\n",
            "datasets/india/UP037.csv\n",
            "datasets/india/KA034.csv\n",
            "datasets/india/CH001.csv\n",
            "datasets/india/PY001.csv\n",
            "datasets/india/HR029.csv\n",
            "datasets/india/BR025.csv\n",
            "datasets/india/UP047.csv\n",
            "datasets/india/RJ019.csv\n",
            "datasets/india/BR032.csv\n",
            "datasets/india/UP016.csv\n",
            "datasets/india/TN001.csv\n",
            "datasets/india/KA033.csv\n",
            "datasets/india/DL031.csv\n",
            "datasets/india/HR003.csv\n",
            "datasets/india/KA032.csv\n",
            "datasets/india/TN010.csv\n",
            "datasets/india/AP002.csv\n",
            "datasets/india/DL040.csv\n",
            "datasets/india/TG003.csv\n",
            "datasets/india/MP017.csv\n",
            "datasets/india/DL028.csv\n",
            "datasets/india/CH002.csv\n",
            "datasets/india/TN007.csv\n",
            "datasets/india/WB004.csv\n",
            "datasets/india/MP008.csv\n",
            "datasets/india/HR022.csv\n",
            "datasets/india/GJ004.csv\n",
            "datasets/india/GJ005.csv\n",
            "datasets/india/UP057.csv\n",
            "datasets/india/DL007.csv\n",
            "datasets/india/MH022.csv\n",
            "datasets/india/MP009.csv\n",
            "datasets/india/OR011.csv\n",
            "datasets/india/HR010.csv\n",
            "datasets/india/TN009.csv\n",
            "datasets/india/KA012.csv\n",
            "datasets/india/DL008.csv\n",
            "datasets/india/RJ015.csv\n",
            "datasets/india/PB001.csv\n",
            "datasets/india/KA021.csv\n",
            "datasets/india/UP043.csv\n",
            "datasets/india/DL039.csv\n",
            "datasets/india/UP048.csv\n",
            "datasets/india/HR009.csv\n",
            "datasets/india/MH007.csv\n",
            "datasets/india/MH009.csv\n",
            "datasets/india/DL019.csv\n",
            "datasets/india/UP006.csv\n",
            "datasets/india/TG005.csv\n",
            "datasets/india/DL010.csv\n",
            "datasets/india/RJ025.csv\n",
            "datasets/india/CG013.csv\n",
            "datasets/india/KA011.csv\n",
            "datasets/india/TG013.csv\n",
            "datasets/india/KA010.csv\n",
            "datasets/india/MH012.csv\n",
            "datasets/india/DL020.csv\n",
            "datasets/india/KL003.csv\n",
            "datasets/india/HR018.csv\n",
            "datasets/india/WB013.csv\n",
            "datasets/india/TN013.csv\n",
            "datasets/india/TN023.csv\n",
            "datasets/india/TN026.csv\n",
            "datasets/india/UP007.csv\n",
            "datasets/india/HR019.csv\n",
            "datasets/india/TN012.csv\n",
            "datasets/india/UP034.csv\n",
            "datasets/india/BR019.csv\n",
            "datasets/india/KA023.csv\n",
            "datasets/india/MP011.csv\n",
            "datasets/india/DL038.csv\n",
            "datasets/india/WB012.csv\n",
            "datasets/india/RJ016.csv\n",
            "datasets/india/DL033.csv\n",
            "datasets/india/KA030.csv\n",
            "datasets/india/DL011.csv\n",
            "datasets/india/KA024.csv\n",
            "datasets/india/BR017.csv\n",
            "datasets/india/DL026.csv\n",
            "datasets/india/KA005.csv\n",
            "datasets/india/BR024.csv\n",
            "datasets/india/UP031.csv\n",
            "datasets/india/KA019.csv\n",
            "datasets/india/BR028.csv\n",
            "datasets/india/GJ009.csv\n",
            "datasets/india/AS003.csv\n",
            "datasets/india/ML001.csv\n",
            "datasets/india/KL005.csv\n",
            "datasets/india/KA009.csv\n",
            "datasets/india/KL006.csv\n",
            "datasets/india/UP025.csv\n",
            "datasets/india/OR008.csv\n",
            "datasets/india/WB006.csv\n",
            "datasets/india/UP041.csv\n",
            "datasets/india/MP020.csv\n",
            "datasets/india/UP040.csv\n",
            "datasets/india/BR034.csv\n",
            "datasets/india/BR013.csv\n",
            "datasets/india/BR023.csv\n",
            "datasets/india/DL012.csv\n",
            "datasets/india/RJ035.csv\n",
            "datasets/india/KA027.csv\n",
            "datasets/india/UP009.csv\n",
            "datasets/india/RJ018.csv\n",
            "datasets/india/KA002.csv\n",
            "datasets/india/MP012.csv\n",
            "datasets/india/BR009.csv\n",
            "datasets/india/TG014.csv\n",
            "datasets/india/UP039.csv\n",
            "datasets/india/MP010.csv\n",
            "datasets/india/UP011.csv\n",
            "datasets/india/UP045.csv\n",
            "datasets/india/MH013.csv\n",
            "datasets/india/MH006.csv\n",
            "datasets/india/SK001.csv\n",
            "datasets/india/KA035.csv\n",
            "datasets/india/UP030.csv\n",
            "datasets/india/DL014.csv\n",
            "datasets/india/MH037.csv\n",
            "datasets/india/HR014.csv\n",
            "datasets/india/AS007.csv\n",
            "datasets/india/KA014.csv\n",
            "datasets/india/BR005.csv\n",
            "datasets/india/BR014.csv\n",
            "datasets/india/stations_info.csv\n",
            "datasets/india/UP008.csv\n",
            "datasets/india/DL027.csv\n",
            "datasets/india/WB001.csv\n",
            "datasets/india/TG006.csv\n",
            "datasets/india/UP038.csv\n",
            "datasets/india/TG010.csv\n",
            "datasets/india/GJ007.csv\n",
            "datasets/india/KA028.csv\n",
            "datasets/india/UP056.csv\n",
            "datasets/india/KL009.csv\n",
            "datasets/india/MZ001.csv\n",
            "datasets/india/NL001.csv\n",
            "datasets/india/MP004.csv\n",
            "datasets/india/MH035.csv\n",
            "datasets/india/JH002.csv\n",
            "datasets/india/DL015.csv\n",
            "datasets/india/MP014.csv\n",
            "datasets/india/PB002.csv\n",
            "datasets/india/MH014.csv\n",
            "datasets/india/MH011.csv\n",
            "datasets/india/AS008.csv\n",
            "datasets/india/KA008.csv\n",
            "datasets/india/UP004.csv\n",
            "datasets/india/TN008.csv\n",
            "datasets/india/BR011.csv\n",
            "datasets/india/KL008.csv\n",
            "datasets/india/BR018.csv\n",
            "datasets/india/UP018.csv\n",
            "datasets/india/UP003.csv\n",
            "datasets/india/UK003.csv\n",
            "datasets/india/MH019.csv\n",
            "datasets/india/RJ034.csv\n",
            "datasets/india/UP036.csv\n",
            "datasets/india/RJ028.csv\n",
            "datasets/india/TN003.csv\n",
            "datasets/india/MP016.csv\n",
            "datasets/india/AS006.csv\n",
            "datasets/india/CG005.csv\n",
            "datasets/india/HR006.csv\n",
            "datasets/india/RJ001.csv\n",
            "datasets/india/DL035.csv\n",
            "datasets/india/DL022.csv\n",
            "datasets/india/BR008.csv\n",
            "datasets/india/BR035.csv\n",
            "datasets/india/TR001.csv\n",
            "datasets/india/MP002.csv\n",
            "datasets/india/RJ026.csv\n",
            "datasets/india/TN018.csv\n",
            "datasets/india/HR012.csv\n",
            "datasets/india/TN005.csv\n",
            "datasets/india/KA007.csv\n",
            "datasets/india/UP042.csv\n",
            "datasets/india/AP005.csv\n",
            "datasets/india/OR001.csv\n",
            "datasets/india/TN019.csv\n",
            "datasets/india/UP046.csv\n",
            "datasets/india/BR015.csv\n",
            "datasets/india/MH015.csv\n",
            "datasets/india/MP013.csv\n",
            "datasets/india/TN020.csv\n",
            "datasets/india/BR001.csv\n",
            "datasets/india/CG014.csv\n",
            "datasets/india/KA025.csv\n",
            "datasets/india/KA013.csv\n",
            "datasets/india/RJ020.csv\n",
            "datasets/india/HR002.csv\n",
            "datasets/india/CG011.csv\n",
            "datasets/india/HR004.csv\n",
            "datasets/india/DL013.csv\n",
            "datasets/india/HR016.csv\n",
            "datasets/india/BR010.csv\n",
            "datasets/india/RJ003.csv\n",
            "datasets/india/GJ010.csv\n",
            "datasets/india/MP001.csv\n",
            "datasets/india/MP005.csv\n",
            "datasets/india/BR003.csv\n",
            "datasets/india/DL009.csv\n",
            "datasets/india/MH004.csv\n",
            "datasets/india/BR029.csv\n",
            "datasets/india/BR002.csv\n",
            "datasets/india/HP001.csv\n",
            "datasets/india/AP007.csv\n",
            "datasets/india/RJ032.csv\n",
            "datasets/india/UP033.csv\n",
            "datasets/india/HR020.csv\n",
            "datasets/india/MH031.csv\n",
            "datasets/india/MH018.csv\n",
            "datasets/india/TN017.csv\n",
            "datasets/india/GJ006.csv\n",
            "datasets/india/DL025.csv\n",
            "datasets/india/KL001.csv\n",
            "datasets/india/CG010.csv\n",
            "datasets/india/OR009.csv\n",
            "datasets/india/MN002.csv\n",
            "datasets/india/TN015.csv\n",
            "datasets/india/TN025.csv\n",
            "datasets/india/WB009.csv\n",
            "datasets/india/MH036.csv\n",
            "datasets/india/HR011.csv\n",
            "datasets/india/UP014.csv\n",
            "datasets/india/AS001.csv\n",
            "datasets/india/TN024.csv\n",
            "datasets/india/MH026.csv\n",
            "datasets/india/AS005.csv\n",
            "datasets/india/CG003.csv\n",
            "datasets/india/DL029.csv\n",
            "datasets/india/UK002.csv\n",
            "datasets/india/UP005.csv\n",
            "datasets/india/RJ005.csv\n",
            "datasets/india/DL023.csv\n",
            "datasets/india/UP015.csv\n",
            "datasets/india/KA006.csv\n",
            "datasets/india/MH033.csv\n",
            "datasets/india/TG012.csv\n",
            "datasets/india/JH001.csv\n",
            "datasets/india/DL002.csv\n",
            "datasets/india/BR026.csv\n",
            "datasets/india/HR015.csv\n",
            "datasets/india/KA018.csv\n",
            "datasets/india/MH021.csv\n",
            "datasets/india/KA031.csv\n",
            "datasets/india/BR016.csv\n",
            "datasets/india/UP023.csv\n",
            "datasets/india/AR001.csv\n",
            "datasets/india/TG011.csv\n",
            "datasets/india/RJ009.csv\n",
            "datasets/india/MH002.csv\n",
            "datasets/india/WB007.csv\n",
            "datasets/india/PB006.csv\n",
            "\n",
            "Estratti:\n",
            "\n",
            "datasets/china/PRSA_Data_Guanyuan_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Wanliu_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Dongsi_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Tiantan_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Nongzhanguan_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Huairou_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Shunyi_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Changping_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Aotizhongxin_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Dingling_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Wanshouxigong_20130301-20170228.csv\n",
            "datasets/china/PRSA_Data_Gucheng_20130301-20170228.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    running_in_colab = True\n",
        "except ImportError:\n",
        "    running_in_colab = False\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "N_JOBS = -1\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "if running_in_colab:\n",
        "    print(\"Running on Google Colab\")\n",
        "    !apt-get update -qq\n",
        "    !apt-get install -qq git-lfs\n",
        "    !git lfs install\n",
        "    !git clone https://github.com/vMxster/Data_Project.git\n",
        "    !cd Data_Project && git lfs pull\n",
        "    zip_india_path   = \"Data_Project/Datasets/dataset_india.zip\"\n",
        "    zip_china_path   = \"Data_Project/Datasets/dataset_china.zip\"\n",
        "else:\n",
        "    print(\"Running locally in Jupyter\")\n",
        "    zip_india_path   = \"Datasets/dataset_india.zip\"\n",
        "    zip_china_path   = \"Datasets/dataset_china.zip\"\n",
        "\n",
        "\n",
        "\n",
        "# India Dataset\n",
        "\n",
        "india_extract_to = \"datasets/india\"\n",
        "os.makedirs(india_extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_india_path, 'r') as z:\n",
        "    z.extractall(india_extract_to)\n",
        "\n",
        "print(\"\\nEstratti:\\n\")\n",
        "for root, _, files in os.walk(india_extract_to):\n",
        "    for f in files:\n",
        "        print(os.path.join(root, f))\n",
        "\n",
        "# Cina Dataset\n",
        "\n",
        "china_extract_to = \"datasets/china\"\n",
        "os.makedirs(china_extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_china_path, 'r') as z:\n",
        "    z.extractall(china_extract_to)\n",
        "\n",
        "print(\"\\nEstratti:\\n\")\n",
        "for root, _, files in os.walk(china_extract_to):\n",
        "    for f in files:\n",
        "        print(os.path.join(root, f))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK_TJF8M2Z4x"
      },
      "source": [
        "# Dataset sulla **Qualità dell'Aria in India**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9MzBRtJ5hlt"
      },
      "source": [
        "Il dataset in questione è stato messo a disposizione dal Central Pollution Control Board (CPCB), l’ente ufficiale del Governo indiano deputato al monitoraggio e alla gestione dell’inquinamento atmosferico, al fine di raccogliere informazioni relative alle condizioni della qualità dell’aria in 453 città indiane nel periodo compreso tra il 2010 e il 2023.\n",
        "\n",
        "Sempre citando la documentazione ufficiale, il dataset permette di indagare su numerose variabili ambientali e parametri atmosferici che includono:\n",
        "\n",
        "- PM10 e PM2.5: concentrazioni di particolato in ug/m³;\n",
        "- CO e CO₂: rispettivamente monossido e anidride carbonica, misurati in vari formati (mg/m³, ppm, ecc.);\n",
        "- NO, NO₂ e NOx: varianti degli ossidi di azoto, riportati in unità adatte (ug/m³, ppb, ppm);\n",
        "- SO₂, NH₃ e altri inquinanti quali Benzene, CH₄, e composti organici come MP-Xylene, Eth-Benzene, O Xylene, e Xylene;\n",
        "- Parametri meteorologici e ambientali quali temperatura, pressione barometrica, umidità relativa, velocità e direzione del vento, radiazione solare e precipitazioni.\n",
        "\n",
        "In aggiunta al dataset principale, è disponibile anche il file “stations_info.csv”, che rappresenta una guida di riferimento per approfondire le informazioni relative alle diverse stazioni di monitoraggio. Questo file include le seguenti intestazioni:\n",
        "\n",
        "- file_name: nome del file associato alla stazione;\n",
        "- state: lo stato in cui è ubicata la stazione;\n",
        "- city: la città in cui la stazione opera;\n",
        "- agency: l’ente responsabile della gestione della stazione;\n",
        "- station_location: dettagli aggiuntivi riguardanti la posizione;\n",
        "- start_month, start_month_num e start_year: informazioni sulla data di inizio della raccolta dati per ciascuna stazione."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IweV0b2IcERD"
      },
      "source": [
        "Si procede alla lettura del file contenente le informazioni relative alle varie stazioni di monitoraggio. Successivamente, verranno rimosse alcune colonne ritenute non necessarie per l'analisi in corso, al fine di semplificare la struttura del dataset ed enfatizzare solo le informazioni rilevanti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gmz4vK9v5mLl",
        "outputId": "faba8338-9b0a-4a4b-8bff-db778090a11c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  file_name           state               city  start_month_num  start_year\n",
              "0     AP001  Andhra Pradesh           Tirupati                7        2016\n",
              "1     AP002  Andhra Pradesh         Vijayawada                5        2017\n",
              "2     AP003  Andhra Pradesh      Visakhapatnam                7        2017\n",
              "3     AP004  Andhra Pradesh  Rajamahendravaram                9        2017\n",
              "4     AP005  Andhra Pradesh          Amaravati               11        2017"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7c994d8-5687-4d27-b2c0-3286b6395d45\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>state</th>\n",
              "      <th>city</th>\n",
              "      <th>start_month_num</th>\n",
              "      <th>start_year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AP001</td>\n",
              "      <td>Andhra Pradesh</td>\n",
              "      <td>Tirupati</td>\n",
              "      <td>7</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AP002</td>\n",
              "      <td>Andhra Pradesh</td>\n",
              "      <td>Vijayawada</td>\n",
              "      <td>5</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AP003</td>\n",
              "      <td>Andhra Pradesh</td>\n",
              "      <td>Visakhapatnam</td>\n",
              "      <td>7</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AP004</td>\n",
              "      <td>Andhra Pradesh</td>\n",
              "      <td>Rajamahendravaram</td>\n",
              "      <td>9</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AP005</td>\n",
              "      <td>Andhra Pradesh</td>\n",
              "      <td>Amaravati</td>\n",
              "      <td>11</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7c994d8-5687-4d27-b2c0-3286b6395d45')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7c994d8-5687-4d27-b2c0-3286b6395d45 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7c994d8-5687-4d27-b2c0-3286b6395d45');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3caacb2f-8d98-4331-af63-df46acdb189d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3caacb2f-8d98-4331-af63-df46acdb189d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3caacb2f-8d98-4331-af63-df46acdb189d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_states",
              "summary": "{\n  \"name\": \"df_states\",\n  \"rows\": 453,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 453,\n        \"samples\": [\n          \"TN002\",\n          \"BR020\",\n          \"TN011\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"Tripura\",\n          \"Meghalaya\",\n          \"Rajasthan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"city\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 241,\n        \"samples\": [\n          \"Sasaram\",\n          \"Chittoor\",\n          \"Greater Noida\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_month_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          6,\n          4,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2010,\n        \"max\": 2023,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          2010,\n          2013,\n          2016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df_states = pd.read_csv(f'{india_extract_to}/stations_info.csv')\n",
        "df_states.drop(columns=['agency', 'station_location', 'start_month'], inplace=True)\n",
        "df_states.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kAYzIaScG-y"
      },
      "source": [
        "Si crea una lista di tutti gli stati presenti nel dataset, assicurandosi di includere ciascun nome una sola volta. Questo passaggio fornisce una visione d’insieme delle regioni coperte dai dati, supportando analisi geografiche e suddivisioni successive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW9vBwAp6Y35",
        "outputId": "c251cd6c-0ddb-436c-cabf-7c81e2c4b878"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar',\n",
              "       'Chhattisgarh', 'Chandigarh', 'Delhi', 'Gujarat',\n",
              "       'Himachal Pradesh', 'Haryana', 'Jharkhand', 'Jammu and Kashmir',\n",
              "       'Karnataka', 'Kerala', 'Maharashtra', 'Meghalaya', 'Manipur',\n",
              "       'Madhya Pradesh', 'Mizoram', 'Nagaland', 'Odisha', 'Punjab',\n",
              "       'Puducherry', 'Rajasthan', 'Sikkim', 'Telangana', 'Tamil Nadu',\n",
              "       'Tripura', 'Uttarakhand', 'Uttar Pradesh', 'West Bengal'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "unique_states = df_states['state'].unique()\n",
        "unique_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZeM8KSFcJkg"
      },
      "source": [
        "Questa funzione consente di raccogliere, integrare e unificare dati relativi alla qualità dell’aria provenienti da più file CSV associati a diverse località di un determinato stato. La strategia prevede innanzitutto l’individuazione automatica di tutti i file relativi ad uno specifico stato, grazie all’utilizzo di un codice identificativo presente all’inizio del nome di ciascun file. Una volta selezionati i file pertinenti, il contenuto di ciascuno viene letto e trasformato in una struttura dati standardizzata. Durante questo processo, viene aggiunta ad ogni insieme di dati un’informazione supplementare che indica la città di appartenenza, garantendo così che ogni record contenga il riferimento geografico completo. Alla fine, i singoli dataset vengono combinati in un’unica struttura dati, offrendo un quadro complessivo che facilita un’analisi omogenea e dettagliata delle tendenze e delle variabili relative alla qualità dell’aria all’interno dello stato esaminato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U8GR4Iib6blW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "def combine_state_df(state_name):\n",
        "    state_code = df_states[df_states['state'] == state_name]['file_name'].iloc[0][:2]\n",
        "\n",
        "    if running_in_colab:\n",
        "        state_files = glob.glob(f'{india_extract_to}/{state_code}*.csv')\n",
        "    else:\n",
        "        state_files = [str(p.as_posix()) for p in Path(india_extract_to).glob(f'{state_code}*.csv')]\n",
        "\n",
        "    combined_df = []\n",
        "    for state_file in state_files:\n",
        "        file_name = state_file.split(f'{india_extract_to}/')[1][:-4]  # remove .csv\n",
        "        file_df = pd.read_csv(state_file)\n",
        "\n",
        "        # Add city and state information\n",
        "        file_df['city'] = df_states[df_states['file_name'] == file_name]['city'].values[0]\n",
        "        file_df['city'] = file_df['city'].astype('string')\n",
        "        file_df['state'] = state_name  # Add the state name\n",
        "\n",
        "        combined_df.append(file_df)\n",
        "\n",
        "    return pd.concat(combined_df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9g8_go06dh-"
      },
      "outputs": [],
      "source": [
        "# Get list of all unique state names\n",
        "unique_states = df_states['state'].unique()\n",
        "\n",
        "# Combine all states' data\n",
        "df_india = pd.concat([combine_state_df(state) for state in unique_states], ignore_index=True)\n",
        "\n",
        "# Optional: show all columns when displaying the DataFrame\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Inspect the combined DataFrame\n",
        "df_india.info()\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnYJ0-80Al5h"
      },
      "source": [
        "Dall’analisi del dataframe risulta che per lo stato di Delhi sono presenti 58 metriche diverse distribuite su un totale di 2.796.171 record."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2WEUSrt6gxV"
      },
      "source": [
        "## Pre-elaborazione dei dati\n",
        "\n",
        "#### Utilizzo di ‘From Date’ come indice temporale\n",
        "\n",
        "Nel dataset sono presenti due colonne di tipo oggetto, `From Date` e `To Date`, che indicano l’inizio e la fine di ciascuna finestra oraria di misurazione. Per gestire efficacemente le serie storiche, trasformiamo `From Date` in un indice datetime, eliminando poi la colonna `To Date`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkt0y_Hc6h9t"
      },
      "outputs": [],
      "source": [
        "def create_dt_index(dataframe):\n",
        "    dataframe = dataframe.drop(columns='To Date')\n",
        "    dataframe['From Date'] = pd.to_datetime(dataframe['From Date'])\n",
        "    dataframe = dataframe.rename(columns={'From Date': 'datetime'})\n",
        "    return dataframe.set_index('datetime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcu280kX-aZg"
      },
      "outputs": [],
      "source": [
        "df_india = create_dt_index(df_india)\n",
        "df_india.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kVtjy2JtGYJ"
      },
      "source": [
        "### Feature Reduction\n",
        "\n",
        "Dall’esame del dataframe emerge che alcune colonne contengono informazioni sovrapposte. Per individuare possibili duplicazioni e fusioni, confronteremo gli andamenti delle medie annuali delle variabili raggruppate. Per ciascun gruppo, aggregheremo i valori per anno e tracceremo un grafico a linee in una griglia, in modo da mettere in evidenza trend comuni e facilitare l’individuazione di correlazioni tra le feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjzqrMES-ntv"
      },
      "outputs": [],
      "source": [
        "def plot_feature_similarities(dataframe, feature_groups, columns=2):\n",
        "    rows = int((len(feature_groups)/columns)//1)\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(13, 4*rows))\n",
        "    fig.tight_layout(pad=3.0)\n",
        "\n",
        "    row_num = 0\n",
        "    col_num = 0\n",
        "    for pos, group in enumerate(feature_groups):\n",
        "        if pos % columns == 0 and pos != 0:\n",
        "            row_num += 1\n",
        "            col_num = 0\n",
        "\n",
        "        for feature in feature_groups[group]:\n",
        "            df_feature = dataframe[dataframe[feature].notnull()][feature]\n",
        "            df_feature = df_feature.groupby([df_feature.index.year]).mean(numeric_only=True)\n",
        "            sns.lineplot(data=df_feature, label=feature, ax=axes[row_num, col_num])\n",
        "        axes[row_num, col_num].set_title(group)\n",
        "        axes[row_num, col_num].set(xlabel=None)\n",
        "        col_num += 1\n",
        "\n",
        "    plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHiUTdxd-u1I"
      },
      "outputs": [],
      "source": [
        "groups = {\n",
        "    'Xylene':            ['Xylene (ug/m3)', 'Xylene ()'],\n",
        "    \"MP-Xylene\":         ['MP-Xylene (ug/m3)', 'MP-Xylene ()'],\n",
        "    'Wind Direction':   [\"WD (degree)\", \"WD (degree C)\", \"WD (deg)\", \"WD ()\"],\n",
        "    'Ozone':             ['Ozone (ug/m3)', 'Ozone (ppb)'],\n",
        "    'Nitrogen Oxides':   ['NOx (ug/m3)', 'NOx (ppb)'],\n",
        "    'Relative humidity':  ['RH (%)', 'RH ()'],\n",
        "    'Solar Radiation': ['SR (W/mt2)', 'SR ()'],\n",
        "    'Air Temperature':  ['AT (degree C)', 'AT ()']\n",
        "}\n",
        "\n",
        "plot_feature_similarities(df_india, groups, columns=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8JHHGJ3AJHn"
      },
      "source": [
        "Procediamo quindi con l’analisi descrittiva delle variabili raggruppate. Dopo aver raccolto in un’unica lista tutte le feature interessate, calcolo per ciascuna statistica di base — media, deviazione standard, valori minimo e massimo — formattando i risultati con tre decimali per migliorarne la leggibilità. Questo step permette di valutare rapidamente la scala e la distribuzione delle variabili, facilitando le decisioni su eventuali fusioni o eliminazioni di feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj8Sy-TU-vUo"
      },
      "outputs": [],
      "source": [
        "all_groups = [item for sublist in list(groups.values()) for item in sublist]\n",
        "df_india[all_groups].describe().map(lambda x: f\"{x:0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6suB5FQdAfSo"
      },
      "source": [
        "Dopo aver esaminato in dettaglio la tabella, avvio la riduzione delle colonne duplicate aggregando quelle che rappresentano la stessa variabile con nomi diversi. Per farlo definisco un dizionario in cui ogni chiave è il nome unificato della variabile e i valori sono le etichette alternative. La funzione itera sul dizionario, trasferendo i valori non nulli dalle colonne secondarie a quella principale e cancellando infine le colonne ridondanti. Questo passaggio semplifica il dataset, eliminando le duplicazioni e facilitando le analisi future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aYiHteu-zb2"
      },
      "outputs": [],
      "source": [
        "reduction_groups = {\n",
        "    \"Xylene (ug/m3)\":    [\"Xylene ()\"],\n",
        "    \"MP-Xylene (ug/m3)\": [\"MP-Xylene ()\"],\n",
        "    \"Benzene (ug/m3)\":   [\"Benzene ()\"],\n",
        "    \"Toluene (ug/m3)\":   [\"Toluene ()\"],\n",
        "    \"SO2 (ug/m3)\":       [\"SO2 ()\"],\n",
        "    \"NOx (ug/m3)\":       [\"NOx (ppb)\"],\n",
        "    \"Ozone (ug/m3)\":     [\"Ozone (ppb)\"],\n",
        "    \"AT (degree C)\":     [\"AT ()\"],\n",
        "    \"WD (degree)\":       [\"WD (degree C)\", \"WD (deg)\", \"WD ()\"],\n",
        "    \"WS (m/s)\":          [\"WS ()\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9anFcFkS-1F_"
      },
      "outputs": [],
      "source": [
        "def merge_columns(dataframe, columns):\n",
        "    for column, cols_to_merge in columns.items():\n",
        "        if column not in dataframe.columns and any(name in dataframe.columns for name in cols_to_merge):\n",
        "            dataframe[column] = np.nan\n",
        "\n",
        "        for col_name in cols_to_merge:\n",
        "            if col_name in dataframe.columns:\n",
        "                dataframe[column] = dataframe[column].fillna(dataframe[col_name])\n",
        "                dataframe = dataframe.drop(columns=[col_name])\n",
        "\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmHd1-CO-2oX"
      },
      "outputs": [],
      "source": [
        "df_india = merge_columns(df_india, reduction_groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hpAPh2ktGYJ"
      },
      "source": [
        "### Verifica dei valori mancanti\n",
        "\n",
        "Il primo passo consiste nel quantificare quanti dati mancanti siano presenti per ciascuna delle feature selezionate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo98OhjSB-2X"
      },
      "outputs": [],
      "source": [
        "df_india.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ccSN4JNCBbQ"
      },
      "outputs": [],
      "source": [
        "df_india = df_india.dropna(how='all')\n",
        "df_india = df_india.dropna(how='all', axis='columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OS0vVAxCGh4"
      },
      "outputs": [],
      "source": [
        "def get_null_info(dataframe):\n",
        "    null_vals = dataframe.isnull().sum()\n",
        "\n",
        "    df_null_vals = pd.concat({'Null Count': null_vals,\n",
        "                              'Percentage of Missing Values (%)': round(null_vals * 100 / len(dataframe), 2)}, axis=1)\n",
        "\n",
        "    return df_null_vals.sort_values(by=['Null Count'], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU3-zyR-CJ9w"
      },
      "outputs": [],
      "source": [
        "df_india_null_info = get_null_info(df_india)\n",
        "\n",
        "plt.figure(figsize=(8, 10))\n",
        "sns.barplot(data=df_india_null_info, x='Percentage of Missing Values (%)', y=df_india_null_info.index, orient='h', color='steelblue')\n",
        "plt.show()\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-29T20:23:06.918541Z",
          "iopub.status.busy": "2024-04-29T20:23:06.91812Z",
          "iopub.status.idle": "2024-04-29T20:23:06.925731Z",
          "shell.execute_reply": "2024-04-29T20:23:06.924162Z",
          "shell.execute_reply.started": "2024-04-29T20:23:06.918508Z"
        },
        "id": "qvmkXNiCtGYK"
      },
      "source": [
        "### Informazioni sul numero dei valori mancanti del dataset\n",
        "\n",
        "Finora abbiamo analizzato solo un singolo stato. Potremmo avere una migliore percezione dei dati mancanti se analizzassimo l'intero Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D740diTEg4Bk"
      },
      "outputs": [],
      "source": [
        "df_india.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8KwOy_vCWhu"
      },
      "outputs": [],
      "source": [
        "def get_overall_ds_info():\n",
        "    features = {}\n",
        "    total_records = 0\n",
        "\n",
        "    for i, state_name in enumerate(unique_states):\n",
        "        clear_output(wait=False)\n",
        "\n",
        "        temp_df = combine_state_df(state_name)\n",
        "        temp_df = create_dt_index(temp_df)\n",
        "        temp_df = temp_df.dropna(how='all')\n",
        "\n",
        "        comparisons = get_null_info(temp_df)\n",
        "\n",
        "        total_records += df_india.shape[0]\n",
        "\n",
        "        for feature in comparisons.index:\n",
        "            if feature in features:\n",
        "                features[feature] += comparisons.loc[[feature]]['Null Count'].values[0]\n",
        "            else:\n",
        "                features[feature] = comparisons.loc[[feature]]['Null Count'].values[0]\n",
        "\n",
        "    ds_null_info = pd.DataFrame.from_dict(features, orient='index', columns=['Null Count'])\n",
        "    ds_null_info['Percentage of Missing Values (%)'] = round(ds_null_info['Null Count'] * 100 / total_records, 2)\n",
        "    ds_null_info = ds_null_info.sort_values(by=['Null Count'], ascending=False)\n",
        "    return ds_null_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPkKypF3CjM_"
      },
      "outputs": [],
      "source": [
        "overall_ds_info = get_overall_ds_info()\n",
        "\n",
        "plt.figure(figsize=(8, 16))\n",
        "sns.barplot(data=overall_ds_info, x='Percentage of Missing Values (%)', y=overall_ds_info.index, orient='h', color='steelblue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0KppCt_tGYK"
      },
      "source": [
        "### Eliminare i valori mancanti per soglia\n",
        "\n",
        "Tornando al dataframe della capitale Delhi, possiamo eliminare le colonne che contengono una certa soglia (cioè > 40%) di valori mancanti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edrptOqUCt8o"
      },
      "outputs": [],
      "source": [
        "threshold = 0.6\n",
        "df_india = df_india.dropna(thresh=df_india.shape[0]*threshold, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abIhhqTUCyYI"
      },
      "outputs": [],
      "source": [
        "get_null_info(df_india)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye-QERh3tGYK"
      },
      "source": [
        "### Analisi esplorativa dei dati\n",
        "\n",
        "Sto raccogliendo le metriche iniziali in diversi gruppi. Ciò consentirà di effettuare confronti migliori."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv9mfA_BEz7w"
      },
      "outputs": [],
      "source": [
        "pollutants = {\n",
        "    'Particulate Matter' : ['PM2.5 (ug/m3)', 'PM10 (ug/m3)'],\n",
        "    'Nitrogen Compounds' : ['NOx (ug/m3)', 'NO (ug/m3)', 'NO2 (ug/m3)', 'NH3 (ug/m3)'],\n",
        "    'Hydrocarbons' : ['Benzene (ug/m3)', 'Eth-Benzene (ug/m3)', 'Xylene (ug/m3)', 'MP-Xylene (ug/m3)', 'O Xylene (ug/m3)', 'Toluene (ug/m3)'],\n",
        "    'Carbon Monoxide': ['CO (mg/m3)'],\n",
        "    'Sulfur Dioxide': ['SO2 (ug/m3)'],\n",
        "    'Ozone Concentration' : ['Ozone (ug/m3)']\n",
        "}\n",
        "\n",
        "other_metrics = {\n",
        "    'Solar Radiation' : ['SR (W/mt2)'],\n",
        "    'Temperatures' : ['Temp (degree C)', 'AT (degree C)'],\n",
        "    'Relative Humidity' : ['RH (%)'],\n",
        "    'Rainfall' : ['RF (mm)'],\n",
        "    'Barometric Pressure' : ['BP (mmHg)'],\n",
        "    'Wind Direction' : ['WD (degree)'],\n",
        "    'Wind Speed' : ['WS (m/s)']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvAWKKqrtGYK"
      },
      "source": [
        "### Frequenze temporali\n",
        "\n",
        "Cominciamo a raggruppare il nostro DataFrame per varie frequenze temporali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzgmZj9SE673"
      },
      "outputs": [],
      "source": [
        "slice_groups = {\n",
        "    'Group by Day':   df_india.groupby(pd.Grouper(freq='1D')).mean(numeric_only=True),\n",
        "    'Group by Month': df_india.groupby(pd.Grouper(freq='1ME')).mean(numeric_only=True),\n",
        "    'Group by Year':  df_india.groupby(pd.Grouper(freq='1YE')).mean(numeric_only=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqCbEOIqE8UR"
      },
      "outputs": [],
      "source": [
        "def plot_features_by_group(features, slice_groups):\n",
        "    for feature in features:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
        "        fig.suptitle(feature)\n",
        "\n",
        "        labels = []\n",
        "        for i, (group, group_df) in enumerate(slice_groups.items()):\n",
        "            data_slice = group_df[group_df.columns.intersection(pollutants[feature])]\n",
        "\n",
        "            if feature == \"Nitrogen Compounds\":\n",
        "                data_slice = data_slice.drop(['NO (ug/m3)', 'NO2 (ug/m3)'], axis=1)\n",
        "\n",
        "            data_slice.plot(kind=\"line\", ax=ax)\n",
        "\n",
        "            for column in data_slice.columns:\n",
        "                labels.append(f'{column} [{group}]')\n",
        "\n",
        "        ax.set(xlabel=None)\n",
        "        ax.legend(labels)\n",
        "        plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8T0BigaE-QY"
      },
      "outputs": [],
      "source": [
        "features_to_plot = ['Particulate Matter', 'Carbon Monoxide', 'Ozone Concentration', 'Nitrogen Compounds']\n",
        "plot_features_by_group(features_to_plot, slice_groups)\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlDvEpT5tGYK"
      },
      "source": [
        "### Analisi stagionale su base annua\n",
        "Dalle metriche selezionate emergono potenziali pattern di tipo stagionale. Per approfondire questa osservazione, eseguiamo un’analisi dettagliata delle variazioni stagionali nell’arco di un anno. Come punto di partenza, prenderemo in considerazione un sottoinsieme di dati relativo al periodo 2019–2020."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5eYo6BgFA53"
      },
      "outputs": [],
      "source": [
        "for feature in features_to_plot:\n",
        "    data_slice = slice_groups['Group by Day'][slice_groups['Group by Day'].columns.intersection(pollutants[feature])]\n",
        "    data_slice.query('datetime > 2019 and datetime < 2020').plot(title=f'{feature} in year 2019-2020', figsize=(12,4)).set(xlabel=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GurXJwUD_fjJ"
      },
      "source": [
        "Si osserva un incremento nei valori di `Particulate Matter`, `Nitrogen Compounds` e `Carbon Monoxide` a partire da ottobre, con un picco che tende a persistere fino circa a marzo. Al contrario, la `Ozone Concentration` mostra un comportamento opposto, raggiungendo i valori massimi indicativamente tra maggio e giugno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHrGqU7QtGYL"
      },
      "source": [
        "### PairPlot\n",
        "Andiamo ad utilizzare il grafico a coppie, che ci consente di visualizzare in modo più chiaro le relazioni bivariate tra le variabili, nonché la distribuzione univariata di ciascuna di esse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK_L7ljNFVsg"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(slice_groups['Group by Month'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uontiBX6tGYO"
      },
      "source": [
        "È evidente una correlazione lineare significativa tra `NOx`, `NO` e `NO2`. Considerando questa relazione, può essere opportuno mantenere esclusivamente la variabile aggregata `NOx` come rappresentazione generale del gruppo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZVr__mOtGYO"
      },
      "source": [
        "### Matrice di correlazione\n",
        "Ora, andiamo ad utilizzare la matrice di correlazione che offre una rappresentazione sintetica ed efficace del grado di associazione lineare tra le diverse variabili del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-b-qifgFcE4"
      },
      "outputs": [],
      "source": [
        "corr = slice_groups['Group by Day'].corr(numeric_only=True).round(2)\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(data=corr, mask=mask, annot=True, cmap=\"rocket_r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7rBOyy1FfB_"
      },
      "outputs": [],
      "source": [
        "corr_target = abs(corr['PM2.5 (ug/m3)'])\n",
        "relevant_features = corr_target[corr_target>0.4]\n",
        "relevant_features.sort_values(ascending=False)\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l03ExqG9_XSa"
      },
      "source": [
        "Il grafico evidenzia diverse correlazioni significative tra le variabili. In particolare:\n",
        "\n",
        "- `NOx` mostra una forte correlazione con le variabili `NO` e `NO2`.\n",
        "- È inoltre evidente una relazione positiva tra `PM2.5` e `NOx`, suggerendo che all’aumentare dei valori di `NOx`, tendono ad aumentare anche i livelli di `PM2.5`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q9lqU0qtGYO"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Eliminazione delle Feature correlate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNTRI3LwFh5v"
      },
      "outputs": [],
      "source": [
        "df_india = df_india.drop(['NO (ug/m3)', 'NO2 (ug/m3)'], axis=1)\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sozitkTJtGYO"
      },
      "source": [
        "### Resampling\n",
        "Poiché il dataframe combinato include misurazioni provenienti da diverse località all'interno dello stesso stato e riferite agli stessi intervalli temporali, è possibile che si verifichino duplicazioni temporali. Dal momento che l’obiettivo è analizzare la qualità dell’aria a livello statale, procederemo con un ricampionamento temporale aggregando i dati mediante media delle misurazioni corrispondenti allo stesso timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ta8VYoFmNZ"
      },
      "outputs": [],
      "source": [
        "df_resampled = (\n",
        "    df_india\n",
        "    .groupby('state')  # Resample within each state\n",
        "    .resample('60min')\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        ")\n",
        "df_resampled = df_resampled.set_index('datetime')\n",
        "df_india=df_resampled.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBARsMqwCukC"
      },
      "source": [
        "### Isolation Forest - Rilevamento e Rimozione degli Outlier\n",
        "In questa sequenza di celle utilizzeremo l'algoritmo Isolation Forest per identificare e rimuovere gli outlier, che rappresentano valori anomali che si discostano in modo significativo dalla distribuzione generale dei dati. La loro presenza può compromettere l’accuratezza delle analisi statistiche e influenzare negativamente le prestazioni dei modelli predittivi. L’identificazione e la rimozione degli outlier consente di ottenere risultati più affidabili e modelli previsionali più robusti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJKQ8AviDeyS"
      },
      "source": [
        "Definiamo le colonne su cui vogliamo applicare l'Isolation Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF3bj1NoDftl"
      },
      "outputs": [],
      "source": [
        "features = ['PM2.5 (ug/m3)', 'CO (mg/m3)', 'Ozone (ug/m3)', 'NOx (ug/m3)']\n",
        "df_india_features = df_india[features].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAcdqgtJDzmg"
      },
      "source": [
        "Creiamo il modello specificando la proporzione di outlier attesi (`contamination`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZNX3_mBD2Ih"
      },
      "outputs": [],
      "source": [
        "iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "iso.fit(df_india_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fShy09vhD3Nt"
      },
      "source": [
        "Usiamo il metodo `predict` per assegnare -1 agli outlier e 1 ai punti normali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNNDaYAED-6z"
      },
      "outputs": [],
      "source": [
        "df_india['anomaly'] = iso.predict(df_india_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5pEmjyLEBQz"
      },
      "source": [
        "Creiamo un nuovo DataFrame senza gli outlier identificati."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS8wjdJyEDN3"
      },
      "outputs": [],
      "source": [
        "df_india_clean = df_india[df_india['anomaly'] == 1].drop(columns='anomaly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZagHinEGEr"
      },
      "source": [
        "Confrontiamo la distribuzione originale e quella ripulita per ogni feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCcboyVGEH_Q"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(4, 2, figsize=(14, 12))\n",
        "for i, col in enumerate(features):\n",
        "    # distribuzione originale\n",
        "    axes[i, 0].hist(df_india[col].dropna(), bins=100)\n",
        "    axes[i, 0].set_title(f\"Originale: {col}\")\n",
        "    # distribuzione pulita\n",
        "    axes[i, 1].hist(df_india_clean[col].dropna(), bins=100)\n",
        "    axes[i, 1].set_title(f\"Pulita: {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df_india = df_india_clean.copy()\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmFhoVqJtGYP"
      },
      "source": [
        "### Gestione dei valori mancanti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkbh-NpTtGYP"
      },
      "outputs": [],
      "source": [
        "get_null_info(df_india)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64de4lrStGYP"
      },
      "outputs": [],
      "source": [
        "numeric_cols = df_india.select_dtypes(include='number').columns\n",
        "\n",
        "df_india[numeric_cols] = df_india[numeric_cols].interpolate(method='pad')\n",
        "df_india[numeric_cols] = df_india[numeric_cols].fillna(df_india[numeric_cols].mean())\n",
        "df_india.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbX4ATiHtGYP"
      },
      "source": [
        "### Arricchimento del Dataset con Caratteristiche Aggiuntive\n",
        "Procediamo con l'ampliamento del nostro dataset, integrando nuove features che possano risultare utili."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "braUbEcytGYP"
      },
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "    df['hour']       = df.index.hour\n",
        "    df['dayofmonth'] = df.index.day\n",
        "    df['dayofweek']  = df.index.dayofweek\n",
        "    df['dayofyear']  = df.index.dayofyear\n",
        "    df['weekofyear'] = df.index.isocalendar().week.astype(\"int64\")\n",
        "    df['month']      = df.index.month\n",
        "    df['quarter']    = df.index.quarter\n",
        "    df['year']       = df.index.year\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5Kq3iFPtGYP"
      },
      "outputs": [],
      "source": [
        "date_features = ['hour', 'dayofmonth', 'dayofweek', 'dayofyear', 'weekofyear', 'month', 'quarter', 'year']\n",
        "df_india = create_features(df_india)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwiSYbFHg4B1"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGyMn04mtGYP"
      },
      "source": [
        "Ora, grazie alle features precedentemente descritte, è semplice visualizzare le diverse metriche. Ad esempio, possiamo esaminare la qualità dell'aria nel corso dei mesi utilizzando un boxplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTZr3MIitGYP"
      },
      "outputs": [],
      "source": [
        "def plot_by_datetime(metric, time_groups):\n",
        "    for time_group in time_groups:\n",
        "        fig, ax = plt.subplots(figsize=(12, 4))\n",
        "        sns.boxplot(data=df_india, x=time_group, y=metric, hue=time_group, palette=\"icefire\", showfliers=False, legend=False)\n",
        "        ax.set_title(f'{metric} by {time_group}')\n",
        "        ax.set(xlabel=time_group)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pae9v3wtGYP"
      },
      "outputs": [],
      "source": [
        "plot_by_datetime('PM2.5 (ug/m3)', ['hour', 'dayofmonth', 'dayofweek', 'weekofyear', 'month', 'quarter', 'year'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egh_BWULuSY7"
      },
      "source": [
        "I grafici mostrano chiaramente che i vari gruppi di date catturano tendenze e informazioni significative. Un punto interessante è che il vettore di feature `dayofweek` potrebbe non essere così rilevante, dato che la distribuzione appare simile per tutti i giorni della settimana. Tuttavia, includeremo comunque tutte queste informazioni nel nostro modello."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsLyrIxOOYW"
      },
      "source": [
        "# Dataset sulla **Qualità dell'Aria in Cina**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRLIHSuJOOYW"
      },
      "source": [
        "Il dataset in questione è stato messo a disposizione dal Beijing Municipal Environmental Monitoring Center, l’ente ufficiale del Governo Cinese deputato al monitoraggio e alla gestione dell’inquinamento atmosferico, al fine di raccogliere informazioni relative alle condizioni della qualità dell’aria nel distretto di Beijing nel periodo compreso tra il 2013 e il 2017.\n",
        "\n",
        "Sempre citando la documentazione ufficiale, il dataset permette di indagare su numerose variabili ambientali e parametri atmosferici che includono:\n",
        "\n",
        "- PM10 e PM2.5: concentrazioni di particolato in ug/m³;\n",
        "- CO: monossido di carbonio (mg/m³);\n",
        "- NO₂: biossido di azoto (ug/m³);\n",
        "- SO₂: anidride solforosa (ug/m³);\n",
        "- O3: concentrazione di Ozono (ug/m^3);\n",
        "- Parametri meteorologici e ambientali quali temperatura, pressione barometrica, temperatura del punto di rugiada, precipitazioni, velocità e direzione del vento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IapxMQ7pOiGv"
      },
      "source": [
        "Unione dei vari CSV in un unico file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67HxvUbzOlno"
      },
      "outputs": [],
      "source": [
        "csv_folder = china_extract_to\n",
        "output_file = os.path.join(china_extract_to, 'combined_dataset.csv')\n",
        "\n",
        "\n",
        "csv_files = [f for f in os.listdir(csv_folder) if f.endswith('.csv')]\n",
        "print(f\"Found {len(csv_files)} CSV files.\")\n",
        "\n",
        "\n",
        "dfs = []\n",
        "for file in csv_files:\n",
        "    path = os.path.join(csv_folder, file)\n",
        "    try:\n",
        "        df_china = pd.read_csv(path)\n",
        "        dfs.append(df_china)\n",
        "        print(f\"Loaded {file} with {len(df_china)} rows.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file}: {e}\")\n",
        "\n",
        "if not dfs:\n",
        "    raise ValueError(\"No CSV files were successfully loaded.\")\n",
        "\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "print(f\"\\nCombined DataFrame has {len(combined_df)} rows and {len(combined_df.columns)} columns.\")\n",
        "\n",
        "\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"\\nCombined CSV saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4sZ0afROOYX"
      },
      "source": [
        "Si procede alla lettura del file contenente le informazioni relative alle varie stazioni di monitoraggio. Successivamente, verranno rimosse alcune colonne ritenute non necessarie per l'analisi in corso, al fine di semplificare la struttura del dataset ed enfatizzare solo le informazioni rilevanti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGN8p1q0GXxg"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv(f'{china_extract_to}/combined_dataset.csv')\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt5YtVgdOOYY"
      },
      "outputs": [],
      "source": [
        "dataframe.drop(columns=['No'], inplace=True)\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZGTwmshOOYY"
      },
      "source": [
        "Si crea una lista di tutti i distretti presenti nel dataset, assicurandosi di includere ciascun nome una sola volta. Questo passaggio fornisce una visione d’insieme delle regioni coperte dai dati, supportando analisi geografiche e suddivisioni successive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4Zes3Y-OOYY"
      },
      "outputs": [],
      "source": [
        "unique_cities = dataframe['station'].unique()\n",
        "unique_cities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqV5Juq7GXxh"
      },
      "source": [
        "Successivamente si crea una vista del numero di entry per ogni distretto. Questo passaggio fornisce una visone dell'uniformità del quantitativo dei dati presenti in ogni distretto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVGwRlvEGXxh"
      },
      "outputs": [],
      "source": [
        "# Quick overview\n",
        "print(f\"[{dataframe['station'].nunique()}] different cities and [{dataframe['station'].count()}] total records available.\")\n",
        "\n",
        "# Get city counts\n",
        "cities = dataframe[\"station\"].value_counts()\n",
        "\n",
        "cities.plot.pie(\n",
        "    labels=[f\"{c}: {p} records\" for c, p in zip(cities.index, cities.values)],\n",
        "    autopct=\"%.1f%%\",\n",
        "    shadow=True,\n",
        "    figsize=(7,7),\n",
        "    title=\"Cities and Record Counts\"\n",
        ");\n",
        "plt.ylabel('');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPsbJtIdOOYZ"
      },
      "source": [
        "Unione delle colonne temporali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPO6e_zwGXxh"
      },
      "outputs": [],
      "source": [
        "dataframe['datetime'] = pd.to_datetime(\n",
        "    dataframe[['year', 'month', 'day', 'hour']],\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "# (Optional) Drop the original columns if you no longer need them\n",
        "dataframe.drop(columns=['year', 'month', 'day', 'hour'], inplace=True)\n",
        "\n",
        "# Preview the result\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QerL3q2eOOYZ"
      },
      "source": [
        "## Pre-elaborazione dei dati\n",
        "\n",
        "#### Utilizzo di ‘datetime’ come indice temporale\n",
        "\n",
        "Per gestire efficacemente le serie storiche, viene utilizzata la colonna `datetime` come indice datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhd28bhcOOYa"
      },
      "outputs": [],
      "source": [
        "dataframe = dataframe.rename(columns={'station': 'state'})\n",
        "dataframe = dataframe.set_index('datetime')\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCGRuFzrOOYa"
      },
      "source": [
        "### Verifica dei valori mancanti\n",
        "\n",
        "Il primo passo consiste nel quantificare quanti dati mancanti siano presenti per ciascuna delle feature selezionate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cx3FbomOOYa"
      },
      "outputs": [],
      "source": [
        "dataframe.isnull().sum().sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS3T2EwPOOYa"
      },
      "outputs": [],
      "source": [
        "df_china=dataframe\n",
        "df_china = df_china.dropna(how='all')\n",
        "df_china = df_china.dropna(how='all', axis='columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcjzKWZEOOYc"
      },
      "outputs": [],
      "source": [
        "def get_null_info(dataframe):\n",
        "    null_vals = dataframe.isnull().sum()\n",
        "\n",
        "    df_null_vals = pd.concat({'Null Count': null_vals,\n",
        "                              'Percentage of Missing Values (%)': round(null_vals * 100 / len(dataframe), 2)}, axis=1)\n",
        "\n",
        "    return df_null_vals.sort_values(by=['Null Count'], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Yf3k_6OOYc"
      },
      "outputs": [],
      "source": [
        "df_null_info = get_null_info(df_china)\n",
        "\n",
        "plt.figure(figsize=(8, 10))\n",
        "sns.barplot(data=df_null_info, x='Percentage of Missing Values (%)', y=df_null_info.index, orient='h', color='steelblue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGdKknNuOOYd"
      },
      "source": [
        "### Eliminare i valori mancanti per soglia\n",
        "\n",
        "Non essendoci colonne al di sopra di una certa soglia (>40%) non viene eliminato nulla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xMmV4P0OOYd"
      },
      "source": [
        "### Analisi esplorativa dei dati\n",
        "\n",
        "Sto raccogliendo le metriche iniziali in diversi gruppi. Ciò consentirà di effettuare confronti migliori."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL8h7XDNOOYd"
      },
      "outputs": [],
      "source": [
        "pollutants = {\n",
        "    'Particulate Matter' : ['PM2.5', 'PM10'],\n",
        "    'Nitrogen Compounds' : ['NO2'],\n",
        "    'Carbon Monoxide': ['CO'],\n",
        "    'Sulfur Dioxide': ['SO2'],\n",
        "    'Ozone Concentration' : ['O3']\n",
        "}\n",
        "other_metrics = {\n",
        "    'Pressure' : ['PRES'],\n",
        "    'Temperatures' : ['TEMP'],\n",
        "    'Dew Point Temperature' : ['DEWP'],\n",
        "    'Rainfall' : ['RAIN'],\n",
        "    'Wind Direction' : ['wd'],\n",
        "    'Wind Speed' : ['WSPM']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3K2MDdhOOYe"
      },
      "source": [
        "### Frequenze temporali\n",
        "\n",
        "Cominciamo a raggruppare il nostro DataFrame per varie frequenze temporali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-r52pfuOOYe"
      },
      "outputs": [],
      "source": [
        "slice_groups = {\n",
        "    'Group by Day':   df_china.groupby(pd.Grouper(freq='1D')).mean(numeric_only=True),\n",
        "    'Group by Month': df_china.groupby(pd.Grouper(freq='1ME')).mean(numeric_only=True),\n",
        "    'Group by Year':  df_china.groupby(pd.Grouper(freq='1YE')).mean(numeric_only=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbuhkBQQOOYe"
      },
      "outputs": [],
      "source": [
        "def plot_features_by_group(features, slice_groups):\n",
        "    for feature in features:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
        "        fig.suptitle(feature)\n",
        "\n",
        "        labels = []\n",
        "        for i, (group, group_df) in enumerate(slice_groups.items()):\n",
        "            data_slice = group_df[group_df.columns.intersection(pollutants[feature])]\n",
        "\n",
        "\n",
        "\n",
        "            data_slice.plot(kind=\"line\", ax=ax)\n",
        "\n",
        "            for column in data_slice.columns:\n",
        "                labels.append(f'{column} [{group}]')\n",
        "\n",
        "        ax.set(xlabel=None)\n",
        "        ax.legend(labels)\n",
        "        plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nSZYMFbOOYe"
      },
      "outputs": [],
      "source": [
        "features_to_plot = ['Particulate Matter', 'Carbon Monoxide', 'Ozone Concentration', 'Nitrogen Compounds']\n",
        "plot_features_by_group(features_to_plot, slice_groups)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p36QPfwmOOYf"
      },
      "source": [
        "### Analisi stagionale su base annua\n",
        "Dalle metriche selezionate emergono potenziali pattern di tipo stagionale. Per approfondire questa osservazione, eseguiamo un’analisi dettagliata delle variazioni stagionali nell’arco di un anno. Come punto di partenza, prenderemo in considerazione un sottoinsieme di dati relativo al periodo 2016–2017."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaNoamFxOOYf"
      },
      "outputs": [],
      "source": [
        "for feature in features_to_plot:\n",
        "    data_slice = slice_groups['Group by Day'][slice_groups['Group by Day'].columns.intersection(pollutants[feature])]\n",
        "    data_slice.query('datetime > 2016 and datetime < 2017').plot(title=f'{feature} in year 2016-2017', figsize=(12,4)).set(xlabel=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcF3iV8hOOYf"
      },
      "source": [
        "Si osserva un incremento nei valori di `Particulate Matter`, `Nitrogen Compounds` e `Carbon Monoxide` a partire da ottobre, con un picco che tende a persistere fino circa a marzo. Al contrario, la `Ozone Concentration` mostra un comportamento opposto, raggiungendo i valori massimi indicativamente tra maggio e giugno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYeGeUWOOOYf"
      },
      "source": [
        "### PairPlot\n",
        "Andiamo ad utilizzare il grafico a coppie, che ci consente di visualizzare in modo più chiaro le relazioni bivariate tra le variabili, nonché la distribuzione univariata di ciascuna di esse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtO39-m9OOYf"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(slice_groups['Group by Month'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNUSK5z_OOYg"
      },
      "source": [
        "### Matrice di correlazione\n",
        "Ora, andiamo ad utilizzare la matrice di correlazione che offre una rappresentazione sintetica ed efficace del grado di associazione lineare tra le diverse variabili del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsL559a3OOYg"
      },
      "outputs": [],
      "source": [
        "corr = slice_groups['Group by Day'].corr(numeric_only=True).round(2)\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(data=corr, mask=mask, annot=True, cmap=\"rocket_r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpvRdX_5OOYg"
      },
      "outputs": [],
      "source": [
        "corr_target = abs(corr['PM2.5'])\n",
        "relevant_features = corr_target[corr_target>0.4]\n",
        "relevant_features.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRhgvEfhOOYh"
      },
      "source": [
        "Il grafico evidenzia diverse correlazioni significative tra le variabili."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgbf1OVmGXxo"
      },
      "source": [
        "Vengono raggruppare le direzioni cardinali del vento mostrando la loro influenza sulla concentrazione di PM2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRMI4OZ3GXxo"
      },
      "outputs": [],
      "source": [
        "# Apply to the dataframe\n",
        "def wind_quadrant_str(direction):\n",
        "    if pd.isna(direction):\n",
        "        return \"Unknown\"\n",
        "    direction = direction.upper()\n",
        "\n",
        "    if direction in ['N', 'NNE', 'NNW']:\n",
        "        return \"N\"\n",
        "    elif direction in ['NE']:\n",
        "        return \"NE\"\n",
        "    elif direction in ['E', 'ENE', 'ESE']:\n",
        "        return \"E\"\n",
        "    elif direction in ['NW']:\n",
        "        return \"NW\"\n",
        "    elif direction in ['S', 'SSE', 'SSW']:\n",
        "        return \"S\"\n",
        "    elif direction in ['SE']:\n",
        "        return \"SE\"\n",
        "    elif direction in ['W', 'WNW', 'WSW']:\n",
        "        return \"W\"\n",
        "    elif direction in ['SW']:\n",
        "        return \"SW\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "# Apply to the dataframe\n",
        "dataframe[\"wind_quadrant\"] = dataframe[\"wd\"].apply(wind_quadrant_str)\n",
        "\n",
        "# Boxplot\n",
        "ax = dataframe.boxplot(\n",
        "    column=\"PM2.5\",\n",
        "    by=\"wind_quadrant\",\n",
        "    showmeans=True,\n",
        "    grid=False\n",
        ")\n",
        "ax.set_ylabel(\"PM2.5 Concentration (µg/m³)\")\n",
        "ax.set_title(\"\")\n",
        "plt.suptitle(\"\")\n",
        "plt.ylim(0, 150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zih7wMyOOYh"
      },
      "source": [
        "### Resampling\n",
        "Poiché il dataframe combinato include misurazioni provenienti da diverse località all'interno dello stesso stato e riferite agli stessi intervalli temporali, è possibile che si verifichino duplicazioni temporali. Dal momento che l’obiettivo è analizzare la qualità dell’aria a livello statale, procederemo con un ricampionamento temporale aggregando i dati mediante media delle misurazioni corrispondenti allo stesso timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za4eYz5pOOYi"
      },
      "outputs": [],
      "source": [
        "df_resampled = (\n",
        "    df_china\n",
        "    .groupby('state')  # Resample within each state\n",
        "    .resample('60min')\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        ")\n",
        "df_resampled = df_resampled.set_index('datetime')\n",
        "df_china=df_resampled.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pny-E8KWLqed"
      },
      "source": [
        "### Isolation Forest - Rilevamento e Rimozione degli Outlier\n",
        "Utilizzeremo l'algoritmo Isolation Forest per identificare e rimuovere automaticamente gli outlier dalle nostre quattro variabili ambientali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuljZTwrMF7G"
      },
      "source": [
        "Definiamo le colonne su cui applicheremo Isolation Forest: `PM2.5`, `CO`, `O3` e `NO2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VvE_9gwMJDx"
      },
      "outputs": [],
      "source": [
        "features = ['PM2.5', 'CO', 'O3', 'NO2']\n",
        "df_china_features = df_china[features].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by8bzYe0MKW8"
      },
      "source": [
        "Impostiamo il parametro `contamination` in base alla percentuale di outlier attesa (qui 1%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_UjIkx1MNZM"
      },
      "outputs": [],
      "source": [
        "iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "iso.fit(df_china_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSeOGv70MRFT"
      },
      "source": [
        "Con `predict`, i valori anomali vengono etichettati con -1, quelli normali con +1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fP1s2NmMUJh"
      },
      "outputs": [],
      "source": [
        "df_china['anomaly'] = iso.predict(df_china_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAtTaShDMWE3"
      },
      "source": [
        "Creiamo un nuovo DataFrame `df_clean` escludendo tutte le righe etichettate come outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-4NiJJ8MYyY"
      },
      "outputs": [],
      "source": [
        "df_china_clean = df_china[df_china['anomaly'] == 1].drop(columns='anomaly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9GzAu8hMa8W"
      },
      "source": [
        "Confrontiamo le distribuzioni originali e quelle ripulite per ciascuna variabile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPh1zUPDMc5u"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(4, 2, figsize=(14, 12))\n",
        "for i, col in enumerate(features):\n",
        "    # istogramma dati originali\n",
        "    axes[i, 0].hist(df_china[col].dropna(), bins=100)\n",
        "    axes[i, 0].set_title(f\"Originale: {col}\")\n",
        "    # istogramma dati puliti\n",
        "    axes[i, 1].hist(df_china_clean[col].dropna(), bins=100)\n",
        "    axes[i, 1].set_title(f\"Pulita: {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df_china = df_china_clean.copy()\n",
        "df_china.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaIppjtBOOYl"
      },
      "source": [
        "### Gestione dei valori mancanti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUglWkBdOOYl"
      },
      "outputs": [],
      "source": [
        "get_null_info(df_china)\n",
        "df_china.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSiuaQgmOOYl"
      },
      "outputs": [],
      "source": [
        "numeric_cols = df_china.select_dtypes(include='number').columns\n",
        "\n",
        "df_china[numeric_cols] = df_china[numeric_cols].interpolate(method='pad')\n",
        "df_china[numeric_cols] = df_china[numeric_cols].fillna(df_china[numeric_cols].mean())\n",
        "\n",
        "df_china.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIBSurMHOOYl"
      },
      "source": [
        "### Arricchimento del Dataset con Caratteristiche Aggiuntive\n",
        "Procediamo con l'ampliamento del nostro dataset, integrando nuove features che possano risultare utili."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSB1DuDQOOYm"
      },
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "    df['hour']       = df.index.hour\n",
        "    df['dayofmonth'] = df.index.day\n",
        "    df['dayofweek']  = df.index.dayofweek\n",
        "    df['dayofyear']  = df.index.dayofyear\n",
        "    df['weekofyear'] = df.index.isocalendar().week.astype(\"int64\")\n",
        "    df['month']      = df.index.month\n",
        "    df['quarter']    = df.index.quarter\n",
        "    df['year']       = df.index.year\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XryI6zLOOYm"
      },
      "outputs": [],
      "source": [
        "date_features = ['hour', 'dayofmonth', 'dayofweek', 'dayofyear', 'weekofyear', 'month', 'quarter', 'year']\n",
        "df_china = create_features(df_china)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krQcOl0ZOOYm"
      },
      "source": [
        "Ora, grazie alle features precedentemente descritte, è semplice visualizzare le diverse metriche. Ad esempio, possiamo esaminare la qualità dell'aria nel corso dei mesi utilizzando un boxplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3UMm0b7OOYm"
      },
      "outputs": [],
      "source": [
        "def plot_by_datetime(metric, time_groups):\n",
        "    for time_group in time_groups:\n",
        "        fig, ax = plt.subplots(figsize=(12, 4))\n",
        "        sns.boxplot(data=df_china, x=time_group, y=metric, hue=time_group, palette=\"icefire\", showfliers=False, legend=False)\n",
        "        ax.set_title(f'{metric} by {time_group}')\n",
        "        ax.set(xlabel=time_group)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD2VzYp4OOYn"
      },
      "outputs": [],
      "source": [
        "plot_by_datetime('PM2.5', ['hour', 'dayofmonth', 'dayofweek', 'weekofyear', 'month', 'quarter', 'year'])\n",
        "df_china.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wfR_PH_uv-x"
      },
      "source": [
        "I grafici mostrano chiaramente che i vari gruppi di date catturano tendenze e informazioni significative. Un punto interessante è che il vettore di feature `dayofweek` potrebbe non essere così rilevante, dato che la distribuzione appare simile per tutti i giorni della settimana. Tuttavia, includeremo comunque tutte queste informazioni nel nostro modello."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xelU5UQqg4CL"
      },
      "source": [
        "# Augmented Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBHLDjsg4CM"
      },
      "source": [
        "Il dataset in questione è stato generato da diversi LLM, tra cui: Deepseek (500 entries), Gemini (3900 entries), Claude (300 entries), Qwen (500 entries), Mistral (500 entries), Meta (400 entries), Grock (100 entries), ChatGpt (200 entries), Hunyuan (350 entries), Tencent (400 entries).\n",
        "Il numero totali di entries utilizzate è proporzionale alla capacità del modello (versione gratuita/prova)  di generare grandi quantitativi di dati.\n",
        "Il dataset è stato generato prendendo come base i dati dell'unione dei dataset precedenti e performando data augmentation con i modelli menzionati in precedenza.\n",
        "Il datasef fa riferimento all'anno 2021 e mantiene le stesse variabili ambientali e parametri atmosferici dei precedenti dataset, rendendolo già omogeneo.\n",
        "I quali includono:\n",
        "\n",
        "- PM10 e PM2.5: concentrazioni di particolato in ug/m³;\n",
        "- CO e CO₂: rispettivamente monossido e anidride carbonica, misurati in vari formati (mg/m³, ppm, ecc.);\n",
        "- NO, NO₂ e NOx: varianti degli ossidi di azoto, riportati in unità adatte (ug/m³, ppb, ppm);\n",
        "- SO₂, NH₃ e altri inquinanti quali Benzene, CH₄, e composti organici come MP-Xylene, Eth-Benzene, O Xylene, e Xylene;\n",
        "- Parametri meteorologici e ambientali quali temperatura, pressione barometrica, umidità relativa, velocità e direzione del vento, radiazione solare e precipitazioni.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ytUJe_cg4CM"
      },
      "source": [
        "Caricamento del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuGhS1vsg4CM"
      },
      "outputs": [],
      "source": [
        "csv_folder = \"datasets/\"\n",
        "df_augmented = pd.read_csv(f'{csv_folder}/Augmented_data.csv')\n",
        "df_augmented.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpYDmBZ2g4CM"
      },
      "source": [
        "Verifico che gli stati generati appaiano una sola volta all'interno del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6saIesTAg4CM"
      },
      "outputs": [],
      "source": [
        "valid_states = df_augmented['state'].unique()\n",
        "valid_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z70_KYnvg4CN"
      },
      "source": [
        "Verifico il numero di entries per stato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z8DMVQog4CN"
      },
      "outputs": [],
      "source": [
        "# Quick overview\n",
        "print(f\"[{df_augmented['state'].nunique()}] different cities and [{df_augmented['state'].count()}] total records available.\")\n",
        "\n",
        "# Get city counts\n",
        "cities = df_augmented[\"state\"].value_counts()\n",
        "\n",
        "cities.plot.pie(\n",
        "    labels=[f\"{c}: {p} records\" for c, p in zip(cities.index, cities.values)],\n",
        "    autopct=\"%.1f%%\",\n",
        "    shadow=True,\n",
        "    figsize=(7,7),\n",
        "    title=\"States and Record Counts\"\n",
        ");\n",
        "plt.ylabel('');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or_Z799Jg4CO"
      },
      "source": [
        "Si nota che i modelli, durante la loro generazione, hanno generato meno stati rispetto ai 32 del dataset omogeneizzato\n",
        "Il quantitativo dei valori generati ammonta a circa 6900 entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL7eApR5g4CO"
      },
      "outputs": [],
      "source": [
        "def plot_by_datetime(metric, time_groups):\n",
        "    for time_group in time_groups:\n",
        "        fig, ax = plt.subplots(figsize=(12, 4))\n",
        "        sns.boxplot(data=df_augmented, x=time_group, y=metric, hue=time_group, palette=\"icefire\", showfliers=False, legend=False)\n",
        "        ax.set_title(f'{metric} by {time_group}')\n",
        "        ax.set(xlabel=time_group)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-nY6_6Ug4CO"
      },
      "outputs": [],
      "source": [
        "plot_by_datetime('PM2.5', ['dayofmonth', 'dayofweek', 'weekofyear', 'month', 'quarter', 'year'])\n",
        "df_china.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZmAT6ojpSh-"
      },
      "source": [
        "# Omogeneizzazione ed Unione dei DataFrame `df_india` e `df_china`\n",
        "\n",
        "In questa sezione andremo a uniformare ed unire la struttura dei due DataFrame, contenenti dati sulla qualità dell'aria dell'India e della Cina, in un unico DataFrame per l'allenamento dei modelli di Regressione.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9vD2UOFprUu"
      },
      "source": [
        "## Rinomina delle colonne in `df_india` per uniformarle a quelle di `df_china`\n",
        "\n",
        "In questa cella rinominiamo alcune colonne di `df_india` in modo che abbiano gli stessi nomi utilizzati in `df_china`.\n",
        "Ad esempio:\n",
        "- `\"PM2.5 (ug/m3)\"` diventerà `\"PM2.5\"`\n",
        "- `\"CO (mg/m3)\"` diventerà `\"CO\"`\n",
        "- `\"Ozone (ug/m3)\"` diventerà `\"O3\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h73bJ1xWpr_q"
      },
      "outputs": [],
      "source": [
        "rename_map_df = {\n",
        "    'PM2.5 (ug/m3)': 'PM2.5',\n",
        "    'CO (mg/m3)': 'CO',\n",
        "    'Ozone (ug/m3)': 'O3',\n",
        "    'NOx (ug/m3)': 'NOx'\n",
        "}\n",
        "\n",
        "df_india.rename(columns=rename_map_df, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AskhNa7VrHDB"
      },
      "source": [
        "## Estrazione delle colonne comuni tra `df_india` e `df_china`\n",
        "\n",
        "Una volta uniformati i nomi delle colonne, cerchiamo l'intersezione tra le colonne dei due DataFrame, cioè quelle che sono presenti in entrambi.\n",
        "Questo ci permetterà di creare due DataFrame omogenei e pronti per essere concatenati.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMZ7zrOwrLyT"
      },
      "outputs": [],
      "source": [
        "common_columns = list(set(df_india.columns) & set(df_china.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QARofnywrTib"
      },
      "source": [
        "## Creazione dei DataFrame `df_india_aligned` e `df_china_aligned`\n",
        "\n",
        "In questa fase creiamo due nuovi DataFrame (`df_aligned` e `df_china_aligned`) che contengono solo le colonne comuni.\n",
        "In questo modo abbiamo una base uniforme su cui allenare i modelli in seguito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPSYDMjpra6j"
      },
      "outputs": [],
      "source": [
        "df_aligned = df_india[common_columns].copy()\n",
        "df_china_aligned = df_china[common_columns].copy()\n",
        "\n",
        "print(\"Colonne comuni (omogeneizzate):\")\n",
        "print(df_aligned.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cDvugQEtvev"
      },
      "source": [
        "## Concatenazione dei DataFrame omogeneizzati\n",
        "\n",
        "Dopo aver uniformato le colonne, possiamo unire i due DataFrame in un unico dataset più grande. Questo ci permette di avere tutti i dati in un'unica struttura per analisi successive. Impostiamo il parametro `ignore_index=True` per riordinare gli indici nel DataFrame concatenato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne2XI1Hwt3Dn"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_aligned, df_china_aligned], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnwJy-xDt9Q-"
      },
      "source": [
        "## Verifica del risultato\n",
        "\n",
        "Stampiamo le dimensioni e alcune righe del nuovo DataFrame concatenato `df` per assicurarci che l’unione sia avvenuta correttamente.\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVMR8y9lt_Xj"
      },
      "outputs": [],
      "source": [
        "print(\"Numero di righe:\", df.shape[0])\n",
        "print(\"Numero di colonne:\", df.shape[1])\n",
        "\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8_ASYnSg4CQ"
      },
      "source": [
        "## Riduzione Sampling\n",
        "\n",
        "Riduciamo il timeframe compreso all'interno del Dataframe al periodo temporale in comune per entrambi i dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZSWpIEeg4CQ"
      },
      "outputs": [],
      "source": [
        "df = df[(df['year'] >= 2015) & (df['year'] <= 2022)]\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9-oclNNg4CR"
      },
      "outputs": [],
      "source": [
        "# 1. Drop the 'hour' column\n",
        "df = df.drop(columns=['hour'])\n",
        "\n",
        "# 2. Group by date and state, and average the pollution values\n",
        "df_daily = (\n",
        "    df.groupby(['year', 'month', 'dayofmonth', 'state'], as_index=False)\n",
        "      .agg({\n",
        "          'PM2.5': 'mean',\n",
        "          'CO': 'mean',\n",
        "          'O3': 'mean',\n",
        "          'dayofweek': 'first',\n",
        "          'quarter': 'first',\n",
        "          'weekofyear': 'first',\n",
        "          'dayofyear': 'first'\n",
        "      })\n",
        ")\n",
        "\n",
        "df_daily.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Z_KX8Lg4CR"
      },
      "source": [
        "## Integrazione Augmented Dataset\n",
        "\n",
        "Inseriamo all'interno del dataframe omogeneizzato i valori generati dai vari llm sostituendo l'anno 2021\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jATLP0T1g4CR"
      },
      "outputs": [],
      "source": [
        "# Split the original df\n",
        "before_2021_unf = df_daily[df_daily['year'] < 2021]\n",
        "after_2021 = df_daily[df_daily['year'] > 2021]\n",
        "\n",
        "before_2021 = before_2021_unf[before_2021_unf['state'].isin(valid_states)].copy()\n",
        "\n",
        "# Concatenate in order: before -> new 2021 -> after\n",
        "df = pd.concat([before_2021, df_augmented, after_2021], ignore_index=True)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70zwr5D7tGYP"
      },
      "source": [
        "# Aggiunta Lag Features\n",
        "\n",
        "Le cosiddette “lag features” consentono di includere nei modelli i valori storici di una variabile, risultando spesso determinanti nelle previsioni grazie al loro elevato potere predittivo. Possiamo inoltre generare lag anche per altre variabili significative, ampliando il contesto informativo del dataset e potenzialmente migliorando la precisione delle stime.\n",
        "\n",
        "Analizzando i boxplot, abbiamo osservato che alcune feature evidenziano trend stagionali o andamenti rilevanti nel tempo. Sulla base di queste evidenze, creeremo lag features mirate per sfruttare al meglio tali pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eBARSxntGYP"
      },
      "outputs": [],
      "source": [
        "def create_lag_features(df):\n",
        "    df = df.copy()\n",
        "    df['pm_lag_1Y'] = df['PM2.5'].shift(365)   # 1 year lag\n",
        "    df['pm_lag_2Y'] = df['PM2.5'].shift(730)   # 2 year lag\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCB-QroxtGYP"
      },
      "outputs": [],
      "source": [
        "lag_features = ['pm_lag_1Y', 'pm_lag_2Y']\n",
        "df = create_lag_features(df)\n",
        "df_daily = create_lag_features(df_daily)\n",
        "df.head()\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsfAIKeLtGYP"
      },
      "source": [
        "A seguito della creazione delle lag features, riscontriamo che i primi record del dataset presentano valori mancanti: ciò è inevitabile, dato che non esistono dati storici precedenti per calcolare i ritardi temporali. È quindi fondamentale gestire con cura questi missing values, poiché molti algoritmi predittivi non possono elaborare dati incompleti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1vOPWt2g4CS"
      },
      "source": [
        "# Creazione Dataset per l'allenamento\n",
        "\n",
        "Esportiamo i due dataset, originale e augmented, per proseguire con l'allenamento nel file successivo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGIaD_iRhvkg"
      },
      "outputs": [],
      "source": [
        "df.to_csv('./original_dataset.csv', sep = ',', index = False)\n",
        "df_daily.to_csv('./augmented_dataset.csv', sep = ',', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if running_in_colab:\n",
        "  google.colab.files.download('./original_dataset.csv')\n",
        "  google.colab.files.download('./augmented_dataset.csv')"
      ],
      "metadata": {
        "id": "xUXMIvaNl7Zy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vnXzcSTc2Z4u",
        "XK_TJF8M2Z4x",
        "aQsLyrIxOOYW",
        "p36QPfwmOOYf",
        "WaIppjtBOOYl",
        "EIBSurMHOOYl",
        "xelU5UQqg4CL",
        "BZmAT6ojpSh-",
        "u9vD2UOFprUu",
        "AskhNa7VrHDB",
        "QARofnywrTib",
        "8cDvugQEtvev",
        "FnwJy-xDt9Q-",
        "70zwr5D7tGYP",
        "r1vOPWt2g4CS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}