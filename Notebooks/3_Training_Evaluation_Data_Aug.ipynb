{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVxxvpq2pGF"
      },
      "source": [
        "# Previsione della Qualità dell'Aria - **Allenamento con Dati Augmentati e Valutazione dei Modelli**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm3Q39LW2Z4t"
      },
      "source": [
        "**Progetto di Data Intensive**  \n",
        "**Autore:** Martin Tomassi, Jacopo Vasi  \n",
        "**Email:** martin.tomassi@studio.unibo.it , jacopo.vasi@studio.unibo.it  \n",
        "**Corso:** Data Intensive, Università di Bologna  \n",
        "**Data:** Aprile 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnXzcSTc2Z4u"
      },
      "source": [
        "## Caricamento dei Datasets ed Import Librerie\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ts2wiQjcg4BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8408c1-39b6-4e05-9913-4abbdf2b3522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "from urllib.request import urlretrieve\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import glob\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    running_in_colab = True\n",
        "except ImportError:\n",
        "    running_in_colab = False\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    mean_squared_log_error,\n",
        "    explained_variance_score,\n",
        "    max_error\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    TimeSeriesSplit,\n",
        "    RandomizedSearchCV,\n",
        "    GridSearchCV,\n",
        "    ParameterSampler\n",
        ")\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression,\n",
        "    Lasso,\n",
        "    Ridge,\n",
        "    ElasticNet\n",
        ")\n",
        "\n",
        "import xgboost as xgb\n",
        "from IPython.display import clear_output\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "\n",
        "%pip install xgboost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "N_JOBS = -1\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/vMxster/Data_Project/main/Datasets/augmented_dataset.csv',\n",
        "                 sep=',',\n",
        "                 quotechar='\"',\n",
        "                 dtype=None,\n",
        "                 parse_dates=True,\n",
        "                 low_memory=False)\n",
        "obj_cols = df.select_dtypes(include=\"object\").columns\n",
        "for col in obj_cols:\n",
        "    df[col] = df[col].astype(\"string\")\n",
        "df = df[(df['year'] >= 2020) & (df['year'] <= 2022)]\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "B-1nc39QrJSv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c8d7f691-7e34-4639-c5e5-4f005d54d663"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   year  month  dayofmonth           state       PM2.5        CO         O3  \\\n",
              "0  2020      1           1  Andhra Pradesh   48.015000  0.869687  54.113646   \n",
              "1  2020      1           1           Assam  130.196667  1.803333  17.892083   \n",
              "2  2020      1           1           Bihar  250.718403  2.375313  23.546944   \n",
              "3  2020      1           1      Chandigarh   62.168333  1.085000   8.322083   \n",
              "4  2020      1           1           Delhi  345.327185  2.906031  24.269346   \n",
              "\n",
              "   dayofweek  quarter  weekofyear  dayofyear   pm_lag_1Y  pm_lag_2Y  \n",
              "0          2        1           1          1   59.107303  69.198630  \n",
              "1          2        1           1          1   33.684854  83.814594  \n",
              "2          2        1           1          1   32.291694  11.342917  \n",
              "3          2        1           1          1   83.392369  80.396250  \n",
              "4          2        1           1          1  104.812348  66.809472  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72dfb271-5373-401c-a974-9f889ce9381d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>dayofmonth</th>\n",
              "      <th>state</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>CO</th>\n",
              "      <th>O3</th>\n",
              "      <th>dayofweek</th>\n",
              "      <th>quarter</th>\n",
              "      <th>weekofyear</th>\n",
              "      <th>dayofyear</th>\n",
              "      <th>pm_lag_1Y</th>\n",
              "      <th>pm_lag_2Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Andhra Pradesh</td>\n",
              "      <td>48.015000</td>\n",
              "      <td>0.869687</td>\n",
              "      <td>54.113646</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>59.107303</td>\n",
              "      <td>69.198630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Assam</td>\n",
              "      <td>130.196667</td>\n",
              "      <td>1.803333</td>\n",
              "      <td>17.892083</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>33.684854</td>\n",
              "      <td>83.814594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Bihar</td>\n",
              "      <td>250.718403</td>\n",
              "      <td>2.375313</td>\n",
              "      <td>23.546944</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>32.291694</td>\n",
              "      <td>11.342917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Chandigarh</td>\n",
              "      <td>62.168333</td>\n",
              "      <td>1.085000</td>\n",
              "      <td>8.322083</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>83.392369</td>\n",
              "      <td>80.396250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>345.327185</td>\n",
              "      <td>2.906031</td>\n",
              "      <td>24.269346</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>104.812348</td>\n",
              "      <td>66.809472</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72dfb271-5373-401c-a974-9f889ce9381d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-72dfb271-5373-401c-a974-9f889ce9381d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-72dfb271-5373-401c-a974-9f889ce9381d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-25af4a59-5a11-44c7-9c98-cc4e10cd82bc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25af4a59-5a11-44c7-9c98-cc4e10cd82bc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-25af4a59-5a11-44c7-9c98-cc4e10cd82bc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 28180,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2020,\n        \"max\": 2022,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2020,\n          2021,\n          2022\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dayofmonth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          28,\n          16,\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"Himachal Pradesh\",\n          \"Rajasthan\",\n          \"Puducherry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PM2.5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.13337424485959,\n        \"min\": 0.0616666666666666,\n        \"max\": 526.0513516014921,\n        \"num_unique_values\": 26395,\n        \"samples\": [\n          76.39728852955324,\n          17.510416666666668,\n          6.4725\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.461561511298943,\n        \"min\": 0.0,\n        \"max\": 8.48,\n        \"num_unique_values\": 20441,\n        \"samples\": [\n          0.6097329695767196,\n          1.9010416666666667,\n          0.8584375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"O3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.65586510273457,\n        \"min\": 0.01,\n        \"max\": 187.2,\n        \"num_unique_values\": 26446,\n        \"samples\": [\n          12.62375,\n          5.245833333333334,\n          31.78198299330556\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dayofweek\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekofyear\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 1,\n        \"max\": 53,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          20,\n          42,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dayofyear\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 104,\n        \"min\": 1,\n        \"max\": 366,\n        \"num_unique_values\": 366,\n        \"samples\": [\n          194,\n          34,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm_lag_1Y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.37174756032832,\n        \"min\": 0.0616666666666666,\n        \"max\": 526.0513516014921,\n        \"num_unique_values\": 26426,\n        \"samples\": [\n          8.841979166666667,\n          62.35050925925926,\n          35.390648301155366\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pm_lag_2Y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.62610130171138,\n        \"min\": 0.0616666666666666,\n        \"max\": 526.0513516014921,\n        \"num_unique_values\": 26447,\n        \"samples\": [\n          16.65625,\n          40.282583333333335,\n          34.015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "j5J-7bPorgC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c47a18a-bf31-423f-be7a-7c0091d158e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 28180 entries, 0 to 28179\n",
            "Data columns (total 13 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   year        28180 non-null  int64  \n",
            " 1   month       28180 non-null  int64  \n",
            " 2   dayofmonth  28180 non-null  int64  \n",
            " 3   state       28180 non-null  string \n",
            " 4   PM2.5       28180 non-null  float64\n",
            " 5   CO          28180 non-null  float64\n",
            " 6   O3          28180 non-null  float64\n",
            " 7   dayofweek   28180 non-null  int64  \n",
            " 8   quarter     28180 non-null  int64  \n",
            " 9   weekofyear  28180 non-null  int64  \n",
            " 10  dayofyear   28180 non-null  int64  \n",
            " 11  pm_lag_1Y   28180 non-null  float64\n",
            " 12  pm_lag_2Y   28180 non-null  float64\n",
            "dtypes: float64(5), int64(7), string(1)\n",
            "memory usage: 2.8 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcIEc735lViq",
        "tags": []
      },
      "source": [
        "# Addestramento modelli\n",
        "A seguito dell'esplorazione e dell'omogeneizzazione dei due dataset, si può procedere all'addestramento dei modelli. I modelli verranno addestrati sulle seguenti feature indipendenti:\n",
        "- `year`: anno della misurazione\n",
        "- `month`: mese dell’anno\n",
        "- `dayofmonth`: giorno del mese\n",
        "- `dayofweek`: giorno della settimana\n",
        "- `dayofyear`: giorno dell’anno\n",
        "- `weekofyear`: settimana dell’anno\n",
        "- `quarter`: trimestre dell’anno\n",
        "- `state`: stato di misurazione\n",
        "- `pm_lag_1W`: PM2.5 ritardato di 1 settimana\n",
        "- `pm_lag_1M`: PM2.5 ritardato di 1 mese\n",
        "- `pm_lag_1Y`: PM2.5 ritardato di 1 anno\n",
        "- `pm_lag_2Y`: PM2.5 ritardato di 2 anni\n",
        "- `co_lag_1W`: CO ritardato di 1 settimana\n",
        "- `co_lag_1M`: CO ritardato di 1 mese\n",
        "- `co_lag_1Y`: CO ritardato di 1 anno\n",
        "- `co_lag_2Y`: CO ritardato di 2 anni\n",
        "- `o3_lag_1W`: O3 ritardato di 1 settimana\n",
        "- `o3_lag_1M`: O3 ritardato di 1 mese\n",
        "- `o3_lag_1Y`: O3 ritardato di 1 anno\n",
        "- `o3_lag_2Y`: O3 ritardato di 2 anni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwC-dywTlViq"
      },
      "source": [
        "La variabile dipendente target dell'addestramento è `PM2.5`, che indica la concentrazione di particelle inquinanti nell'aria con un diametro inferiore a 2,5 micron (μm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GveBLWbLlViq"
      },
      "source": [
        "## Preparazione Dataset\n",
        "Per garantire un confronto equo tra tutti i modelli, alcuni dei quali non supportano i valori mancanti generati dalle lag features, elimineremo tutte le righe che li contengono. Va però tenuto presente che così facendo perdiamo un anno di dati storici. Modelli come XGBoost di scikit-learn sono in grado di gestire internamente i missing value e potrebbero beneficiarne; tuttavia, per mantenere omogenee le condizioni di allenamento, applichiamo il drop completo dei NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2u4WO1bmlViq"
      },
      "outputs": [],
      "source": [
        "target = 'PM2.5'\n",
        "lag_features = ['pm_lag_1Y', 'pm_lag_2Y', 'pm_lag_1M', 'pm_lag_1W','co_lag_1Y', 'co_lag_2Y', 'co_lag_1M', 'co_lag_1W','o3_lag_1Y', 'o3_lag_2Y', 'o3_lag_1M', 'o3_lag_1W']\n",
        "date_features = ['dayofmonth', 'dayofweek', 'dayofyear', 'weekofyear', 'month', 'quarter', 'year', 'state']\n",
        "predictors = date_features + lag_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "efbb3yDvuaDc"
      },
      "outputs": [],
      "source": [
        "def create_train_test_sets(dataframe, split, replace_na=False, method='none'):\n",
        "    dataframe = dataframe.copy()\n",
        "\n",
        "    if replace_na and method == 'zeros':\n",
        "      dataframe = dataframe.fillna(0)\n",
        "    elif replace_na and method == 'drop':\n",
        "      dataframe = dataframe.dropna(how='any')\n",
        "\n",
        "    train_set, test_set = np.split(dataframe, [int(len(dataframe) * split)])\n",
        "    return train_set[predictors], test_set[predictors], train_set[target], test_set[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5_dgAHmhuh2i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "4a49560a-1c83-462e-eafa-e76618eaf8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['pm_lag_1M', 'pm_lag_1W', 'co_lag_1Y', 'co_lag_2Y', 'co_lag_1M', 'co_lag_1W', 'o3_lag_1Y', 'o3_lag_2Y', 'o3_lag_1M', 'o3_lag_1W'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3174486825.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m X_train, X_test, y_train, y_test = create_train_test_sets(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreplace_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-2225303691.py\u001b[0m in \u001b[0;36mcreate_train_test_sets\u001b[0;34m(dataframe, split, replace_na, method)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['pm_lag_1M', 'pm_lag_1W', 'co_lag_1Y', 'co_lag_2Y', 'co_lag_1M', 'co_lag_1W', 'o3_lag_1Y', 'o3_lag_2Y', 'o3_lag_1M', 'o3_lag_1W'] not in index\""
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = create_train_test_sets(\n",
        "    df,\n",
        "    split=0.8,\n",
        "    replace_na=True,\n",
        "    method='drop'\n",
        ")\n",
        "\n",
        "# Resetta gli indici dei risultati di create_train_test_sets eliminando l’indice precedente,\n",
        "# in modo da partire da zero e avere indici continui\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "X = pd.concat([X_train, X_test])\n",
        "y = pd.concat([y_train, y_test])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "3Mz9jup_67xf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "dd36a4f3-a258-4d80-e978-9f8cd38d9d3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-581197040.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.info()"
      ],
      "metadata": {
        "id": "gl2OvBn56_PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBCT7crrf9A"
      },
      "source": [
        "Nel dataset ci sono sia feature numeriche che categoriche. <br>\n",
        "Per le numeriche è necessario applicare una normalizzazione dei dati, i quali avrebbero altrimenti valori su scale molto diverse che renderebbero più difficile la convergenza del modello. <br>\n",
        "Per poter utilizzare le variabili categoriche nell'addestramento di un modello di regressione si usa un OneHotEncoder, creando nuove colonne binarie per ciascuno dei valori ammissibili dalla variabile categorica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcK3Ma4545-a"
      },
      "outputs": [],
      "source": [
        "categorical_features = X.select_dtypes(include=[\"string\"]).columns.tolist()\n",
        "numerical_features   = [c for c in X.columns if c not in categorical_features]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    # Standardizza tutte le colonne numeriche\n",
        "    (\"numeric\",    StandardScaler(),    numerical_features),\n",
        "    # One‑hot encoding di 'state', ignorando nuovi stati in predict\n",
        "    (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axPc4fYaKiDT"
      },
      "source": [
        "Inizializzazione della lista per raccogliere le metriche dopo ogni training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LfV4nYJKiPx"
      },
      "outputs": [],
      "source": [
        "all_scores = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_bIhHTRKAzQ"
      },
      "source": [
        "## Valutazione delle Prestazioni dei Modelli Allenati\n",
        "\n",
        "Dopo l'allenamento di ciascun modello di Machine Learning e Deep Learning, utilizziamo la funzione `get_estimator_scores` e `get_torch_estimator_scores` per calcolare diverse metriche di valutazione, includendo per le principali anche gli **Intervalli di Confidenza al 95%** (CI95%) stimati tramite bootstrap resampling.\n",
        "\n",
        "Le metriche calcolate sono:\n",
        "\n",
        "1. **$R^2$ (Coefficiente di Determinazione)**  \n",
        "   Misura quanto bene un modello riesce a spiegare la variabilità della variabile dipendente. Il valore di $R^2$ varia da 0 (nessuna capacità predittiva) a 1 (predizione perfetta).  \n",
        "   Un $R^2_{\\text{test}}$ significativamente inferiore rispetto a $R^2_{\\text{train}}$ indica possibile overfitting.  \n",
        "   *(Più alto è, meglio è.)*\n",
        "\n",
        "2. **Root Mean Squared Error (RMSE)**  \n",
        "   Misura la deviazione standard degli errori di previsione (la radice quadrata del MSE). Penalizza maggiormente gli errori più grandi.  \n",
        "   *(Più basso è, meglio è.)*\n",
        "\n",
        "3. **Mean Absolute Error (MAE)**  \n",
        "   Rappresenta la media delle differenze assolute tra i valori reali e quelli predetti. È meno sensibile agli outlier rispetto all’RMSE.  \n",
        "   *(Più basso è, meglio è.)*\n",
        "\n",
        "4. **Mean Absolute Percentage Error (MAPE)**  \n",
        "   Misura la precisione percentuale di un sistema di previsione. Indica, in media, quanto si discosta una previsione rispetto al valore reale.  \n",
        "   *(Più basso è, meglio è.)*\n",
        "\n",
        "5. **Mean Squared Logarithmic Error (MSLE)**  \n",
        "   Utile quando si vuole penalizzare maggiormente gli errori relativi per valori piccoli e avere tolleranza su valori grandi.  \n",
        "   *(Più basso è, meglio è.)*\n",
        "\n",
        "6. **Explained Variance Score**  \n",
        "   Misura la proporzione della varianza spiegata dal modello, simile a $R^2$, ma può essere più informativo in caso di regressione non lineare.  \n",
        "   *(Più alto è, meglio è.)*\n",
        "\n",
        "7. **Max Error**  \n",
        "   Misura l’errore assoluto massimo commesso in una previsione.  \n",
        "   *(Più basso è, meglio è.)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zthJQ-LRL-8J"
      },
      "outputs": [],
      "source": [
        "def bootstrap_ci(metric_fn, y_true, y_pred, n_bootstraps=1000, alpha=0.05):\n",
        "    y_true_arr = np.asarray(y_true)\n",
        "    y_pred_arr = np.asarray(y_pred)\n",
        "    vals = []\n",
        "    for _ in range(n_bootstraps):\n",
        "        if running_in_colab:\n",
        "            idx = np.random.randint(0, len(y_true), len(y_true))\n",
        "            vals.append(metric_fn(y_true[idx], y_pred[idx]))\n",
        "        else:\n",
        "            idx = np.random.randint(0, len(y_true), len(y_true))\n",
        "            vals.append(metric_fn(y_true_arr[idx], y_pred_arr[idx]))\n",
        "\n",
        "    low = np.percentile(vals, 100 * (alpha/2))\n",
        "    high = np.percentile(vals, 100 * (1 - alpha/2))\n",
        "    return low, high\n",
        "\n",
        "def get_estimator_scores(model_name, model):\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2_tr = model.score(X_train, y_train)\n",
        "    r2_te = r2_score(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "    msle = mean_squared_log_error(y_test, np.maximum(y_pred, 0))\n",
        "    evs = explained_variance_score(y_test, y_pred)\n",
        "    me = max_error(y_test, y_pred)\n",
        "\n",
        "    rmse_low, rmse_high = bootstrap_ci(lambda a,b: np.sqrt(mean_squared_error(a, b)), y_test, y_pred)\n",
        "    mae_low, mae_high = bootstrap_ci(mean_absolute_error, y_test, y_pred)\n",
        "    mape_low, mape_high = bootstrap_ci(mean_absolute_percentage_error, y_test, y_pred)\n",
        "\n",
        "    all_scores.append([\n",
        "        model_name, r2_tr, r2_te,\n",
        "        rmse, rmse_low, rmse_high,\n",
        "        mae, mae_low, mae_high,\n",
        "        mape, mape_low, mape_high,\n",
        "        msle, evs, me\n",
        "    ])\n",
        "\n",
        "def predict_torch(model, X_tensor, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(X_tensor.to(device))\n",
        "    out_cpu = out.detach().cpu().numpy()\n",
        "    # Se il modello restituisce shape (N,1), appiattiamo a (N,)\n",
        "    if out_cpu.ndim == 2 and out_cpu.shape[1] == 1:\n",
        "        return out_cpu.ravel()\n",
        "    return out_cpu\n",
        "\n",
        "def get_torch_estimator_scores(model_name, model,\n",
        "                               X_train, y_train,\n",
        "                               X_test, y_test,\n",
        "                               device):\n",
        "    y_pred_train = predict_torch(model, X_train, device)\n",
        "    y_pred_test  = predict_torch(model, X_test,  device)\n",
        "\n",
        "    y_train_np = y_train.detach().cpu().numpy().ravel()\n",
        "    y_test_np  = y_test.detach().cpu().numpy().ravel()\n",
        "\n",
        "    r2_tr = r2_score(y_train_np, y_pred_train)\n",
        "    r2_te = r2_score(y_test_np,  y_pred_test)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_test))\n",
        "    rmse_low, rmse_high = bootstrap_ci(\n",
        "        lambda a, b: np.sqrt(mean_squared_error(a, b)),\n",
        "        y_test_np, y_pred_test\n",
        "    )\n",
        "\n",
        "    mae = mean_absolute_error(y_test_np, y_pred_test)\n",
        "    mae_low, mae_high = bootstrap_ci(mean_absolute_error, y_test_np, y_pred_test)\n",
        "    mape = mean_absolute_percentage_error(y_test_np, y_pred_test)\n",
        "    mape_low, mape_high = bootstrap_ci(mean_absolute_percentage_error,\n",
        "                                       y_test_np, y_pred_test)\n",
        "    msle = mean_squared_log_error(y_test_np, np.maximum(y_pred_test, 0))\n",
        "    evs = explained_variance_score(y_test_np, y_pred_test)\n",
        "    me = max_error(y_test_np, y_pred_test)\n",
        "\n",
        "    all_scores.append([\n",
        "        model_name, r2_tr, r2_te,\n",
        "        rmse, rmse_low, rmse_high,\n",
        "        mae, mae_low, mae_high,\n",
        "        mape, mape_low, mape_high,\n",
        "        msle, evs, me\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOMRnqcslVir"
      },
      "source": [
        "## Regressione lineare\n",
        "Il modello più semplice da addestrare è la regressione lineare, senza filtri polinomiali o regolarizzazioni. Grazie all'elevato numero di dati usato per l'addestramento, si possono ottenere degli ottimi risultati, anche se migliorabili, già con questo primo modello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSsOPwFNlVir"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"regr\"   , LinearRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVduMAAHlVir"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "get_estimator_scores(\"lin_reg\", model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP2p8Z1ClVit",
        "tags": []
      },
      "source": [
        "## Regressione polinomiale\n",
        "Per provare ad ottenere risultati migliori, vengono introdotte le feature polinomiali, che aggiungono nuove feature di grado superiore. Si esclude il bias, che consiste nel valore dell'intercetta, e si escludono i prodotti tra le diverse feature, per esempio i doppi prodotti in una regressione polinomiale di grado 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n3i5NhmlVit"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
        "    (\"regr\"   , LinearRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txYsQh_mlViw"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "get_estimator_scores(\"poly_reg\", model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCa0HV05lVix",
        "tags": []
      },
      "source": [
        "## Regressione LASSO\n",
        "La regressione LASSO è un'ottima tecnica per selezionare le feature più importanti, poichè la discesa del gradiente si ferma su un vertice di un ipercubo centrato sull'origine, quindi azzera i parametri delle variabili meno rilevanti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO8UrLjylVix"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"regr\", Lasso(alpha=1, max_iter=10000))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW41o83llVix"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcl2wO-FlVi0"
      },
      "source": [
        "## Regressione Ridge\n",
        "Proviamo la regolarizzazione con feature polinomiali per ridurre un eventuale overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUItTWw3lVi0"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"poly\",   PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
        "    (\"regr\", Ridge(alpha=1, max_iter=10000))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mqZrg_UlVi0"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXwTWsC2lVi3"
      },
      "source": [
        "## Regressione Elastic Net\n",
        "Questa regressione unisce la regolarizzazione LASSO e la regolarizzazione Ridge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE5PXsuelVi3"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"poly\",   PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
        "    (\"regr\", ElasticNet(alpha=0.2, l1_ratio=0.1))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD_KhUiylVi3"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSqJT3J-lVi4",
        "tags": []
      },
      "source": [
        "## Regressione con funzioni kernel\n",
        "Per ovviare ai problemi di prestazioni dei modelli con feature polinomiali, usiamo il kernel trick per evitare di creare un numero elevato di feature aggiuntive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFj50pJolVi4",
        "tags": []
      },
      "source": [
        "## Funzioni kernel polinomiali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xa639FtlVi4"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"regr\",  KernelRidge(alpha=1, kernel=\"poly\", degree=10))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnFxvlfPlVi4"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4RNo3BclVi5"
      },
      "source": [
        "## Funzioni kernel gaussiane"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-78ywm3XlVi5"
      },
      "source": [
        "Testiamo anche funzioni kernel diverse, ad esempio RBF (_radial basis function_). <br>\n",
        "La funzione RBF ha la forma di una gaussiana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T5BL022lVi5"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"regr\",  KernelRidge(alpha=1, kernel=\"rbf\", gamma=0.01))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlJ8BI7UlVi6"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sELWDyRclVi6"
      },
      "source": [
        "## Alberi decisionali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVoeA9SklVi6"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"tree\", DecisionTreeRegressor(max_depth=4, random_state=42))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOZPvS-MlVi7"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM7vW1UglVi8"
      },
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eieu-i1lVi9"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"tree\", RandomForestRegressor(max_samples=0.2, max_features=\"sqrt\", n_estimators=200, max_depth=None, n_jobs=-1))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB_F1HTDlVi9"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9HoQcolVi9"
      },
      "source": [
        "Possiamo ricavare le 10 feature più importanti per la Random Forest, ovvero le variabili che sono state più utilizzate nella creazione degli alberi decisionali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQVy9ajGlVi9"
      },
      "outputs": [],
      "source": [
        "pd.Series(model.named_steps[\"tree\"].feature_importances_, preprocessor.get_feature_names_out(X_train.columns)).sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xPxYN70lVi_"
      },
      "source": [
        "## XGBoost\n",
        "XGBoost crea una foresta di alberi in cui ogni albero utilizza gli errori commessi dall'albero precedente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A4je0oplVjA"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"xgb\", XGBRegressor(objective='reg:squarederror', n_estimators=200))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A3332KolVjA"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUxDT_-nlVjA"
      },
      "source": [
        "Possiamo ricavare le 5 feature più importanti per XGBoost, ovvero le variabili che sono state più utilizzate nella creazione degli alberi decisionali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us6Z4OmqlVjA"
      },
      "outputs": [],
      "source": [
        "pd.Series(model.named_steps[\"xgb\"].feature_importances_, preprocessor.get_feature_names_out(X_train.columns)).sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "xlwH7ptOzQwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device utilizzato: {device}\")\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_sizes, dropout):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = input_dim\n",
        "        for hs in hidden_sizes:\n",
        "            layers.append(nn.Linear(dim, hs))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            dim = hs\n",
        "        layers.append(nn.Linear(dim, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "FKVjVeNVzTPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search con Early Stopping\n",
        "\n",
        "Questo blocco di codice implementa una procedura completa di **Random Search** per la selezione di iperparametri di modelli di regressione in PyTorch, integrando una strategia di **Early Stopping** per migliorare l'efficienza del training.\n",
        "\n",
        "* La classe `EarlyStopper` consente di interrompere l'addestramento anticipatamente se la loss di validazione non migliora per un numero di epoche definito (`patience`), riducendo il rischio di overfitting e velocizzando l'ottimizzazione.\n",
        "* Le funzioni `train_epoch` ed `eval_loss` gestiscono rispettivamente il training e la valutazione della loss media su un dataset.\n",
        "* La funzione principale `random_search` esegue una **Cross-Validation**, dove:\n",
        "  * Il ciclo valuta le prestazioni generali del modello su diversi split train/test."
      ],
      "metadata": {
        "id": "CcnVXcxkzVxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=3, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, val_loss):\n",
        "        # Se la loss migliora (di almeno min_delta), resettiamo il counter\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            # Se la loss non migliora da 'patience' epoche, dobbiamo fermarci\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for Xb, yb in loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_loss(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            total_loss += criterion(model(Xb), yb).item() * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def random_search(model_builder, param_dist, dataset,\n",
        "                  n_iter=10, cv_folds=5,\n",
        "                  early_patience=5,\n",
        "                  early_min_delta=1e-4):\n",
        "    train_keys = ['lr', 'batch_size', 'max_epochs']\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_params, best_train_params = None, None\n",
        "    best_model = None\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
        "\n",
        "    print(\"Avvio Random Search con Time Series Cross Validation...\")\n",
        "\n",
        "    for param_id, params in enumerate(ParameterSampler(param_dist, n_iter=n_iter, random_state=RANDOM_STATE)):\n",
        "        print(f\"Testing parameter set {param_id+1}/{n_iter}\")\n",
        "\n",
        "        model_params = {k: v for k, v in params.items() if k not in train_keys}\n",
        "        train_params = {k: v for k, v in params.items() if k in train_keys}\n",
        "        val_losses = []\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(range(len(dataset)))):\n",
        "            print(f\"  Fold {fold_idx+1}/{cv_folds}\")\n",
        "\n",
        "            sub_train = Subset(dataset, train_idx)\n",
        "            val_set = Subset(dataset, val_idx)\n",
        "            train_loader = DataLoader(sub_train, batch_size=train_params['batch_size'], shuffle=True)\n",
        "            val_loader = DataLoader(val_set, batch_size=train_params['batch_size'], shuffle=False)\n",
        "\n",
        "            model = model_builder(**model_params).to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=train_params['lr'])\n",
        "            stopper = EarlyStopper(patience=early_patience, min_delta=early_min_delta)\n",
        "\n",
        "            for epoch in range(train_params['max_epochs']):\n",
        "                train_epoch(model, train_loader, optimizer, nn.MSELoss())\n",
        "                val_loss = eval_loss(model, val_loader, nn.MSELoss())\n",
        "\n",
        "                if epoch % 10 == 0:  # Print every 10 epochs\n",
        "                    print(f\"    Epoch {epoch}: val_loss = {val_loss:.6f}\")\n",
        "\n",
        "                if stopper.early_stop(val_loss):\n",
        "                    print(f\"    Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "            final_val_loss = eval_loss(model, val_loader, nn.MSELoss())\n",
        "            val_losses.append(final_val_loss)\n",
        "\n",
        "        mean_val = np.mean(val_losses)\n",
        "        print(f\"  Mean validation loss: {mean_val:.6f}\")\n",
        "\n",
        "        if mean_val < best_val_loss:\n",
        "            best_val_loss = mean_val\n",
        "            best_model_params = model_params\n",
        "            best_train_params = train_params\n",
        "            best_model = model\n",
        "            print(f\"  New best validation loss: {best_val_loss:.6f}\")\n",
        "\n",
        "    print(f\"\\nBest validation loss: {best_val_loss:.6f}\")\n",
        "    return best_model, best_model_params, best_train_params"
      ],
      "metadata": {
        "id": "Iumk8NyQzZco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esecuzione del Random Search su MLP\n",
        "\n",
        "In questa sezione viene eseguita la **ricerca di iperparametri tramite Random Search** per **MLP (Multi-Layer Perceptron)**.\n",
        "\n",
        "#### Definizione degli spazi degli iperparametri:\n",
        "\n",
        "* `mlp_param_dist`: contiene combinazioni di dimensioni dei layer nascosti, tassi di dropout, learning rate, batch size e numero massimo di epoche per il training del modello MLP.\n"
      ],
      "metadata": {
        "id": "OVD5006Oza85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# (Training + Validation) Dataset\n",
        "full_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "input_dim = X_tensor.shape[1]\n",
        "mlp_param_dist = {\n",
        "    'input_dim': [input_dim],\n",
        "    'hidden_sizes': [(32,32), (64,64), (128,)],\n",
        "    'dropout': [0.0, 0.2, 0.5],\n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'batch_size': [32, 64],\n",
        "    'max_epochs': [50]\n",
        "}\n",
        "best_model, model_params, train_params = random_search(\n",
        "    lambda **p: MLP(**p), mlp_param_dist, full_dataset\n",
        ")\n",
        "\n",
        "X_train_tensor = torch.stack([full_dataset[i][0] for i in range(len(full_dataset))])\n",
        "y_train_tensor = torch.stack([full_dataset[i][1] for i in range(len(full_dataset))])\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "get_torch_estimator_scores(\"MLP\", best_model,\n",
        "                           X_train_tensor.to(device), y_train_tensor.to(device),\n",
        "                           X_test_tensor, y_test_tensor,\n",
        "                           device)"
      ],
      "metadata": {
        "id": "lHuw7xi2zemy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uIugxCaMC9R"
      },
      "source": [
        "# Confronto Visivo delle Prestazioni dei Modelli\n",
        "\n",
        "La funzione `plot_estimator_scores(scores)` consente di visualizzare in modo sintetico ed intuitivo le metriche di valutazione di tutti i modelli allenati. I grafici generati permettono un confronto diretto tra le prestazioni su diverse metriche chiave:\n",
        "\n",
        "- **R² Score**: confronta le prestazioni sul training set e sul test set per identificare possibili fenomeni di overfitting.\n",
        "- **RMSE (Root Mean Squared Error)**: evidenzia la variabilità degli errori di previsione, penalizzando fortemente gli outlier.\n",
        "- **MAE (Mean Absolute Error)**: mostra la media dell’errore assoluto commesso da ciascun modello.\n",
        "- **MAPE (Mean Absolute Percentage Error)**: fornisce un’indicazione dell’errore medio in termini percentuali rispetto ai valori reali.\n",
        "- **MSLE (Mean Squared Logarithmic Error)**: utile nei casi in cui gli errori relativi siano più importanti degli assoluti, o in presenza di target con ordini di grandezza diversi.\n",
        "- **Explained Variance Score**: indica la proporzione della varianza spiegata dal modello (simile a $R^2$).\n",
        "- **Max Error**: evidenzia il peggior errore assoluto commesso su un'osservazione.\n",
        "- **Intervalli di Confidenza (CI95%)**: per le metriche `RMSE`, `MAE` e `MAPE` viene stimato un intervallo di confidenza al 95% tramite bootstrap resampling, al fine di rappresentare l'incertezza statistica associata alla metrica.\n",
        "\n",
        "Questa visualizzazione finale è utile per trarre conclusioni sulla bontà predittiva di ciascun modello e guidare la scelta del miglior approccio da adottare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-29T20:38:15.404812Z",
          "iopub.status.busy": "2024-04-29T20:38:15.403803Z",
          "iopub.status.idle": "2024-04-29T20:38:15.421022Z",
          "shell.execute_reply": "2024-04-29T20:38:15.419432Z",
          "shell.execute_reply.started": "2024-04-29T20:38:15.404772Z"
        },
        "id": "5vxh_8pW9qkC"
      },
      "outputs": [],
      "source": [
        "def plot_estimator_scores(scores):\n",
        "    melted_r2 = scores[['model', 'r2_train', 'r2_test']]\n",
        "    melted_r2 = melted_r2.rename(columns={'r2_train':'train','r2_test':'test'})\n",
        "    melted_r2 = melted_r2.melt(id_vars='model', var_name='set', value_name='score')\n",
        "\n",
        "    fig, axs = plt.subplots(3, 3, figsize=(18, 14))\n",
        "    fig.tight_layout(pad=4)\n",
        "\n",
        "    sns.barplot(data=melted_r2, x='score', y='model', hue='set', ax=axs[0,0])\n",
        "    axs[0,0].set_title('R2 Score')\n",
        "    axs[0,0].legend(loc='lower right')\n",
        "\n",
        "    axs[0,1].set_title('RMSE ± CI95%')\n",
        "    for _, row in scores.iterrows():\n",
        "        axs[0,1].barh(row['model'], row['rmse'],\n",
        "                      xerr=[[row['rmse']-row['rmse_low']], [row['rmse_high']-row['rmse']]], capsize=5)\n",
        "\n",
        "    axs[0,2].set_title('MAE ± CI95%')\n",
        "    for _, row in scores.iterrows():\n",
        "        axs[0,2].barh(row['model'], row['mae'],\n",
        "                      xerr=[[row['mae']-row['mae_low']], [row['mae_high']-row['mae']]], capsize=5)\n",
        "\n",
        "    axs[1,0].set_title('MAPE ± CI95%')\n",
        "    for _, row in scores.iterrows():\n",
        "        axs[1,0].barh(row['model'], row['mape'],\n",
        "                      xerr=[[row['mape']-row['mape_low']], [row['mape_high']-row['mape']]], capsize=5)\n",
        "\n",
        "    axs[1,1].set_title('MSLE')\n",
        "    for _, row in scores.iterrows():\n",
        "        axs[1,1].barh(row['model'], row['msle'])\n",
        "\n",
        "    axs[1,2].set_title('Explained Variance')\n",
        "    for _, row in scores.iterrows():\n",
        "        axs[1,2].barh(row['model'], row['explained_var'])\n",
        "    axs[1,2].set_xlim(0,1)\n",
        "\n",
        "    axs[2,0].set_title('Max Error')\n",
        "    for _, row in scores.iterrows():\n",
        "        axs[2,0].barh(row['model'], row['max_error'])\n",
        "\n",
        "    axs[2,1].axis('off')\n",
        "    axs[2,2].axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-29T20:38:15.424047Z",
          "iopub.status.busy": "2024-04-29T20:38:15.422783Z",
          "iopub.status.idle": "2024-04-29T20:38:17.073862Z",
          "shell.execute_reply": "2024-04-29T20:38:17.072755Z",
          "shell.execute_reply.started": "2024-04-29T20:38:15.424005Z"
        },
        "id": "RTHU3LQ49qkD"
      },
      "outputs": [],
      "source": [
        "estimator_scores_df = pd.DataFrame(\n",
        "    all_scores,\n",
        "    columns = [\n",
        "        'model','r2_train','r2_test',\n",
        "        'rmse','rmse_low','rmse_high',\n",
        "        'mae','mae_low','mae_high',\n",
        "        'mape','mape_low','mape_high',\n",
        "        'msle','explained_var','max_error'\n",
        "    ]\n",
        ")\n",
        "plot_estimator_scores(estimator_scores_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mvVxxvpq2pGF",
        "vnXzcSTc2Z4u",
        "pcIEc735lViq",
        "H_bIhHTRKAzQ",
        "xOMRnqcslVir",
        "YP2p8Z1ClVit",
        "tCa0HV05lVix",
        "Mcl2wO-FlVi0",
        "aXwTWsC2lVi3",
        "OSqJT3J-lVi4",
        "VFj50pJolVi4",
        "p4RNo3BclVi5",
        "sELWDyRclVi6",
        "sM7vW1UglVi8",
        "7xPxYN70lVi_",
        "xlwH7ptOzQwq",
        "CcnVXcxkzVxm",
        "5uIugxCaMC9R"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}